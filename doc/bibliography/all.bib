
@inproceedings{le_building_2013,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6639343},
  timestamp = {2016-04-20T13:41:25Z},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2013 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Le, Quoc V.},
  urldate = {2016-04-20},
  date = {2013},
  pages = {8595--8598},
  file = {Le_2013_Building_high-level_features_using_large_scale_unsupervised_learning.pdf:/Users/jschmolling/Dropbox/ebooks/Le_2013_Building_high-level_features_using_large_scale_unsupervised_learning.pdf:application/pdf},
  groups = {unsupervised}
}

@online{staatsbibliothek_zu_berlin_digitalisierte_2016,
  title = {Digitalisierte {{Sammlungen}} Der {{Staatsbibliothek}} Zu {{Berlin}}: {{Startseite}}},
  url = {http://digital.staatsbibliothek-berlin.de/},
  timestamp = {2016-07-21T00:22:33Z},
  author = {{\{Staatsbibliothek zu Berlin\}}},
  urldate = {2016-07-21},
  date = {2016},
  file = {Digitalisierte Sammlungen der Staatsbibliothek zu Berlin\: Startseite:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/NNJUFW92/digital.staatsbibliothek-berlin.de.html:text/html}
}

@online{wilhelm_busch_max_1865,
  title = {Max Und {{Moritz}} : Eine {{Bubengeschichte}} in Sieben {{Streichen}}},
  url = {http://digital.staatsbibliothek-berlin.de/werkansicht?PPN=PPN644058722&PHYSID=PHYS_0007&DMDID=DMDLOG_0001},
  timestamp = {2016-07-21T00:52:46Z},
  author = {{Wilhelm Busch}},
  urldate = {2016-07-21},
  date = {1865},
  file = {Digitalisierte Sammlungen der Staatsbibliothek zu Berlin\: Werkansicht - PPN644058722 - Übersicht mit Inhaltsverzeichnis:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DJMZQJEH/werkansicht.html:text/html}
}

@online{rodrigo_benenson_classification_2016,
  title = {Classification Datasets Results},
  url = {https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html},
  timestamp = {2016-07-21T09:59:34Z},
  author = {{Rodrigo Benenson}},
  urldate = {2016-07-21},
  date = {2016},
  file = {Classification datasets results:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/EG8JSTE4/classification_datasets_results.html:text/html}
}

@article{vinyals_show_2014,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  url = {http://arxiv.org/abs/1411.4555},
  shorttitle = {Show and {{Tell}}},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  timestamp = {2016-07-21T10:15:51Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4555},
  primaryClass = {cs},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  urldate = {2016-07-21},
  date = {2014-11-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Vinyals_et_al_2014_Show_and_Tell.pdf:/Users/jschmolling/Dropbox/ebooks/Vinyals_et_al_2014_Show_and_Tell.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/FXSJIW82/1411.html:text/html}
}

@online{_staatsbibliothek_2016,
  title = {Staatsbibliothek Zu {{Berlin}}},
  url = {http://staatsbibliothek-berlin.de/},
  timestamp = {2016-07-21T12:59:54Z},
  urldate = {2016-07-21},
  date = {2016},
  file = {Startseite | Staatsbibliothek zu Berlin:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DJ42ZG6F/staatsbibliothek-berlin.de.html:text/html}
}

@book{clough_best_2013,
  title = {Best of {{Both Worlds}}: {{Museums}}, {{Libraries}}, and {{Archives}} in the {{Digital Age}}},
  shorttitle = {Best of {{Both Worlds}}},
  abstract = {Wayne Clough, Secretary of the Smithsonian Institution, asks museums a fundamental question: "How can we prepare ourselves to reach the generation of digital natives who bring a huge appetite-and aptitude-for the digital world?" His thoughts on how the Smithsonian is tackling this issue and how others have fared in museums and libraries around the world are the subject of a new e-book, Best of Both Worlds: Museums, Libraries, and Archives in a Digital Age.The e-book begins with a summary of what has already taken place in libraries and archives-documents once available only in the stacks or back rooms are now available, often for free, through the Internet. The Library of Congress and the National Archives, along with thousands of local libraries around the country, have led the way in digitizing two-dimensional objects.For museums, the digital world presents a bigger challenge. Clough cites several reasons for this. First, and most obvious: Producing images of three-dimensional objects is more complicated than taking a picture of a page and adding some data. Most museums, including the Smithsonian, previously have not had high levels of technical expertise and equipment. Adding to the challenge is the simple fact that collections are built with exhibitions in mind rather than open access on computers.With 137 million objects in its care, how did the Smithsonian begin the process of digitizing its vast collections to make them accessible to the millions of people who do not visit the museums in person? Clough describes the journey that began in 2009 with setting priorities for what would be digitized-a total of 14 million objects."The physical museum offers visitors the opportunity to experience the real object and share their impressions with family and friends," Clough said. "Digital access can then provide limitless opportunities for engagement and lifelong learning."Museums do not give degrees but they do provide informal education through their research, scholarship and exhibitions, both real and virtual. Clough sees museums gradually moving beyond showcasing collections to engaging the public online where the "visitors" can sort out and access the objects they find most interesting and then interact directly with the museums.Education has always been at the core of the Smithsonian. Today, the Smithsonian offers materials and lesson plans that meet state standards for K-12 curricula; online national summits for teens on subjects such as the 1961 Freedom Riders, the environment and the 1930s Dust Bowl; the Collections Search Center website; and apps such as Leafsnap that allow people to take a picture of a tree's leaf and have it identified in seconds. One of the most visible examples of informal education is the Smithsonian's website, www.seriouslyamazing.com, which draws people in with fun questions and, with another click, takes them deeper into the subject. For example, when the question "What European colonizer is still invading the U.S. today?" is clicked on, the answer, earthworms, appears along with further in-depth information on worms from environmental researchers.Museums have moved slowly-frequently project by project-into the brave new digital world, according to Clough. There are many good reasons for this cautious path, but "the time for toe-dipping is ending for museums, since they cannot stand aloof from the rising tide of information convergence."Clough concludes his 75-page online book with this thought: "While digital technology poses great challenges, it also offers great possibilities. For the Smithsonian and our nation's other museums, libraries and archives, today is a time when we can serve the role our founders envisioned for the educational systems of our republic. We can help all the people,},
  pagetotal = {130},
  timestamp = {2016-07-21T19:32:35Z},
  langid = {english},
  publisher = {{Smithsonian Books}},
  author = {Clough, G. Wayne},
  date = {2013-09-17}
}

@article{razavian_cnn_2014,
  title = {{{CNN Features}} Off-the-Shelf: An {{Astounding Baseline}} for {{Recognition}}},
  url = {http://arxiv.org/abs/1403.6382},
  shorttitle = {{{CNN Features}} Off-the-Shelf},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the $\backslash$overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the $\backslash$overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the $\backslash$overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  timestamp = {2016-07-24T23:46:05Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.6382},
  primaryClass = {cs},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  urldate = {2016-07-24},
  date = {2014-03-23},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Razavian_et_al_2014_CNN_Features_off-the-shelf.pdf:/Users/jschmolling/Dropbox/ebooks/Razavian_et_al_2014_CNN_Features_off-the-shelf.pdf:application/pdf}
}

@article{girshick_region-based_2016,
  title = {Region-Based Convolutional Networks for Accurate Object Detection and Segmentation},
  volume = {38},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7112511},
  timestamp = {2016-07-24T23:39:00Z},
  number = {1},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  urldate = {2016-07-24},
  date = {2016},
  pages = {142--158},
  keywords = {segmentation},
  file = {Girshick_et_al_2016_Region-based_convolutional_networks_for_accurate_object_detection_and.pdf:/Users/jschmolling/Dropbox/ebooks/Girshick_et_al_2016_Region-based_convolutional_networks_for_accurate_object_detection_and.pdf:application/pdf}
}

@article{ren_faster_2015,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  url = {http://arxiv.org/abs/1506.01497},
  shorttitle = {Faster {{R}}-{{CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  timestamp = {2016-07-25T07:58:31Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  urldate = {2016-07-25},
  date = {2015-06-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Ren_et_al_2015_Faster_R-CNN.pdf:/Users/jschmolling/Dropbox/ebooks/Ren_et_al_2015_Faster_R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/8KW3TTPZ/1506.html:text/html},
  groups = {region-proposal}
}

@inreference{_digital_2016,
  title = {Digital Humanities},
  rights = {Creative Commons Attribution-ShareAlike License},
  url = {https://de.wikipedia.org/w/index.php?title=Digital_Humanities&oldid=156118661},
  abstract = {Das Fach Digital Humanities (deutsch: „digitale Geisteswissenschaften“) umfasst die Anwendung von computergestützten Verfahren und die systematische Verwendung von digitalen Ressourcen in den Geistes- und Kulturwissenschaften. Es handelt sich um ein interdisziplinär ausgerichtetes Fach, dessen Vertreter sowohl durch eine traditionelle Ausbildung in den Geistes- und Kulturwissenschaften ausgewiesen sind wie durch ihre Vertrautheit mit einer Reihe von einschlägigen Konzepten, Verfahren und Standards der Informatik. In Deutschland sind das insbesondere Forscher der Computerphilologie, der Historischen Fachinformatik, der Informationswissenschaft und der Computerlinguistik. Typische Arbeits- und Forschungsfelder sind z.B. digitale Editionen, quantitative Textanalyse, Visualisierung komplexer Datenstrukturen oder die Theorie digitaler Medien.},
  timestamp = {2016-07-25T08:29:40Z},
  langid = {german},
  booktitle = {Wikipedia},
  urldate = {2016-07-25},
  date = {2016-07-13},
  note = {Page Version ID: 156118661},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/GSQ4E2QK/index.html:text/html}
}

@article{harley_evaluation_2015,
  title = {Evaluation of {{Deep Convolutional Nets}} for {{Document Image Classification}} and {{Retrieval}}},
  url = {http://arxiv.org/abs/1502.07058},
  abstract = {This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.},
  timestamp = {2016-07-25T08:41:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.07058},
  primaryClass = {cs},
  author = {Harley, Adam W. and Ufkes, Alex and Derpanis, Konstantinos G.},
  urldate = {2016-07-25},
  date = {2015-02-25},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Harley_et_al_2015_Evaluation_of_Deep_Convolutional_Nets_for_Document_Image_Classification_and.pdf:/Users/jschmolling/Dropbox/ebooks/Harley_et_al_2015_Evaluation_of_Deep_Convolutional_Nets_for_Document_Image_Classification_and.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/EF48WJAS/1502.html:text/html}
}

@article{redmon_you_2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  url = {http://arxiv.org/abs/1506.02640},
  shorttitle = {You {{Only Look Once}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  timestamp = {2016-07-26T15:22:11Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02640},
  primaryClass = {cs},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  urldate = {2016-07-26},
  date = {2015-06-08},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Redmon_et_al_2015_You_Only_Look_Once.pdf:/Users/jschmolling/Dropbox/ebooks/Redmon_et_al_2015_You_Only_Look_Once.pdf:application/pdf},
  groups = {region-proposal}
}

@article{muhlberger_handwritten_????,
  title = {Handwritten {{Text Recognition}} ({{HTR}}) of {{Historical Documents}} as a {{Shared Task}} for {{Archivists}}, {{Computer Scientists}} and {{Humanities Scholars}}. {{The Model}} of a {{Transcription}} \& {{Recognition Platform}} ({{TRP}})},
  url = {http://www.academia.edu/download/34967812/muehlberger_histoinformatics-2.pdf},
  timestamp = {2016-07-28T17:21:08Z},
  author = {Mühlberger, Günter and Colutto, Sebastian and Kahle, Philip},
  urldate = {2016-07-28}
}

@article{achanta_slic_2012,
  title = {{{SLIC Superpixels Compared}} to {{State}}-of-the-{{Art Superpixel Methods}}},
  volume = {34},
  issn = {0162-8828, 2160-9292},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6205760},
  doi = {10.1109/TPAMI.2012.120},
  timestamp = {2016-08-07T15:18:15Z},
  number = {11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Achanta, R. and Shaji, A. and Smith, K. and Lucchi, A. and Fua, P. and Süsstrunk, Sabine},
  urldate = {2016-08-07},
  date = {2012-11},
  pages = {2274--2282},
  file = {Achanta_et_al_2012_SLIC_Superpixels_Compared_to_State-of-the-Art_Superpixel_Methods.pdf:/Users/jschmolling/Dropbox/ebooks/Achanta_et_al_2012_SLIC_Superpixels_Compared_to_State-of-the-Art_Superpixel_Methods.pdf:application/pdf}
}

@article{zhao_stacked_2015,
  title = {Stacked {{What}}-{{Where Auto}}-Encoders},
  url = {http://arxiv.org/abs/1506.02351},
  abstract = {We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.},
  timestamp = {2016-08-10T12:35:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02351},
  primaryClass = {cs, stat},
  author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and LeCun, Yann},
  urldate = {2016-08-10},
  date = {2015-06-08},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {Zhao_et_al_2015_Stacked_What-Where_Auto-encoders.pdf:/Users/jschmolling/Dropbox/ebooks/Zhao_et_al_2015_Stacked_What-Where_Auto-encoders.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/5AV6Z67U/1506.html:text/html},
  groups = {maybe}
}

@article{dundar_convolutional_2015,
  title = {Convolutional {{Clustering}} for {{Unsupervised Learning}}},
  url = {http://arxiv.org/abs/1511.06241},
  abstract = {The task of labeling data for training deep neural networks is daunting and tedious, requiring millions of labels to achieve the current state-of-the-art results. Such reliance on large amounts of labeled data can be relaxed by exploiting hierarchical features via unsupervised learning techniques. In this work, we propose to train a deep convolutional network based on an enhanced version of the k-means clustering algorithm, which reduces the number of correlated parameters in the form of similar filters, and thus increases test categorization accuracy. We call our algorithm convolutional k-means clustering. We further show that learning the connection between the layers of a deep convolutional neural network improves its ability to be trained on a smaller amount of labeled data. Our experiments show that the proposed algorithm outperforms other techniques that learn filters unsupervised. Specifically, we obtained a test accuracy of 74.1\% on STL-10 and a test error of 0.5\% on MNIST.},
  timestamp = {2016-08-10T12:35:20Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06241},
  primaryClass = {cs},
  author = {Dundar, Aysegul and Jin, Jonghoon and Culurciello, Eugenio},
  urldate = {2016-08-10},
  date = {2015-11-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Dundar_et_al_2015_Convolutional_Clustering_for_Unsupervised_Learning.pdf:/Users/jschmolling/Dropbox/ebooks/Dundar_et_al_2015_Convolutional_Clustering_for_Unsupervised_Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/34MPWQDG/1511.html:text/html},
  groups = {maybe}
}

@online{_kyamagu/js-segment-annotator_????,
  title = {Kyamagu/Js-Segment-Annotator},
  url = {https://github.com/kyamagu/js-segment-annotator},
  abstract = {js-segment-annotator - Javascript image annotation tool based on image segmentation.},
  timestamp = {2016-08-10T12:36:51Z},
  journaltitle = {GitHub},
  urldate = {2016-08-10},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/J5UTA9HU/js-segment-annotator.html:text/html}
}

@article{girshick_rich_2013,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  url = {http://arxiv.org/abs/1311.2524},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde{}rbg/rcnn.},
  timestamp = {2016-11-23T15:20:49Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.2524},
  primaryClass = {cs},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  urldate = {2016-11-23},
  date = {2013-11-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Girshick_et_al_2013_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation.pdf:/Users/jschmolling/Dropbox/ebooks/Girshick_et_al_2013_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/PDCN5GNP/1311.html:text/html},
  groups = {region-proposal}
}

@article{bojanowski_enriching_2016,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  url = {http://arxiv.org/abs/1607.04606},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpus quickly. We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.},
  timestamp = {2017-02-14T13:34:00Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.04606},
  primaryClass = {cs},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  urldate = {2017-02-14},
  date = {2016-07-15},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning},
  file = {Bojanowski_et_al_2016_Enriching_Word_Vectors_with_Subword_Information.pdf:/Users/jschmolling/Dropbox/ebooks/Bojanowski_et_al_2016_Enriching_Word_Vectors_with_Subword_Information.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/333UPX68/1607.html:text/html},
  groups = {words}
}

@article{joulin_bag_2016,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  url = {http://arxiv.org/abs/1607.01759},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore\textasciitilde{}CPU, and classify half a million sentences among\textasciitilde{}312K classes in less than a minute.},
  timestamp = {2017-02-14T13:34:19Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.01759},
  primaryClass = {cs},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  urldate = {2017-02-14},
  date = {2016-07-06},
  keywords = {Computer Science - Computation and Language},
  file = {Joulin_et_al_2016_Bag_of_Tricks_for_Efficient_Text_Classification.pdf:/Users/jschmolling/Dropbox/ebooks/Joulin_et_al_2016_Bag_of_Tricks_for_Efficient_Text_Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/QMXKCSEM/1607.html:text/html},
  groups = {words}
}

@online{_gensim:_????,
  title = {Gensim: Topic Modelling for Humans},
  url = {https://radimrehurek.com/gensim/index.html},
  shorttitle = {Gensim},
  abstract = {Efficient topic modelling in Python},
  timestamp = {2017-02-14T13:35:08Z},
  urldate = {2017-02-14},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/IZZNEN2N/index.html:text/html},
  groups = {words}
}

@online{_facebookresearch/fasttext:_????,
  title = {Facebookresearch/{{fastText}}: {{Library}} for Fast Text Representation and Classification.},
  url = {https://github.com/facebookresearch/fastText},
  timestamp = {2017-02-14T13:35:54Z},
  urldate = {2017-02-14},
  file = {facebookresearch/fastText\: Library for fast text representation and classification.:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/GPRTMWNR/fastText.html:text/html},
  groups = {words}
}

@thesis{felsberg_training_2015,
  title = {Training von {{Rekurrenten Neuronalen Netzen}} Mittels {{GPGPU}}-{{Computing}}},
  url = {https://sisis.rz.htw-berlin.de/InfoGuideClient/login.do},
  timestamp = {2017-05-01T08:45:39Z},
  author = {Felsberg, Norbert},
  urldate = {2017-05-01},
  date = {2015},
  file = {webOPAC der HTW:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/5P2I45KZ/login.html:text/html},
  groups = {htw-masterarbeiten}
}

@thesis{bielen_entwicklung_2015,
  title = {Entwicklung Eines Lernfähigen {{Alogrithmus}} Zur {{Auto}}-{{Kaufberatung}}},
  url = {https://sisis.rz.htw-berlin.de/InfoGuideClient/search.do?methodToCall=submit&CSId=7651N18S6381d24b8205ee20884e89c7890a060ce4b673ff&methodToCallParameter=submitSearch&searchCategories%5B0%5D=-1&searchString%5B0%5D=Bielen%2C+Sebastian&callingPage=searchParameters&selectedViewBranchlib=94&searchRestrictionID%5B0%5D=6&searchRestrictionValue1%5B0%5D=&searchRestrictionID%5B1%5D=5&searchRestrictionValue1%5B1%5D=&searchRestrictionID%5B2%5D=4&searchRestrictionValue1%5B2%5D=&searchRestrictionID%5B3%5D=2&searchRestrictionValue1%5B3%5D=},
  timestamp = {2017-05-01T08:48:13Z},
  author = {Bielen, Sebastian},
  urldate = {2017-05-01},
  date = {2015},
  file = {webOPAC der HTW:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/KGHXDR62/search.html:text/html},
  groups = {htw-masterarbeiten}
}

@thesis{ufer_metauberwachtes_2016,
  title = {Metaüberwachtes {{Lernen}} Neuronaler {{Netze}} Durch {{Modifizierung}} Des {{Backpropagation Algorithmus}}},
  url = {https://sisis.rz.htw-berlin.de/InfoGuideClient/singleHit.do?methodToCall=showHit&curPos=1&identifier=-1_S_IG.274.20737},
  timestamp = {2017-05-01T08:58:42Z},
  author = {Ufer, Moritz},
  urldate = {2017-05-01},
  date = {2016},
  file = {Ufer_2016_Metaüberwachtes_Lernen_neuronaler_Netze_durch_Modifizierung_des_Backpropagation.pdf:/Users/jschmolling/Dropbox/ebooks/Ufer_2016_Metaüberwachtes_Lernen_neuronaler_Netze_durch_Modifizierung_des_Backpropagation.pdf:application/pdf;webOPAC der HTW:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/CJV82R6F/singleHit.html:text/html},
  groups = {htw-masterarbeiten}
}

@article{simonyan_very_2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  timestamp = {2017-05-22T10:14:16Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  author = {Simonyan, Karen and Zisserman, Andrew},
  urldate = {2017-05-22},
  date = {2014-09-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Simonyan_Zisserman_2014_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition.pdf:/Users/jschmolling/Dropbox/ebooks/Simonyan_Zisserman_2014_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/WIBPNC5S/1409.html:text/html},
  groups = {nn-feature-clustering}
}

@inproceedings{kang_convolutional_2014,
  title = {Convolutional Neural Networks for Document Image Classification},
  url = {http://ieeexplore.ieee.org/abstract/document/6977258/},
  timestamp = {2017-05-15T12:45:09Z},
  booktitle = {Pattern {{Recognition}} ({{ICPR}}), 2014 22nd {{International Conference}} On},
  publisher = {{IEEE}},
  author = {Kang, Le and Kumar, Jayant and Ye, Peng and Li, Yi and Doermann, David},
  urldate = {2017-05-15},
  date = {2014},
  pages = {3168--3172},
  file = {Kang_et_al_2014_Convolutional_neural_networks_for_document_image_classification.pdf:/Users/jschmolling/Dropbox/ebooks/Kang_et_al_2014_Convolutional_neural_networks_for_document_image_classification.pdf:application/pdf}
}

@article{shi_end--end_2015,
  title = {An {{End}}-to-{{End Trainable Neural Network}} for {{Image}}-Based {{Sequence Recognition}} and {{Its Application}} to {{Scene Text Recognition}}},
  url = {http://arxiv.org/abs/1507.05717},
  abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
  timestamp = {2017-06-07T08:52:04Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.05717},
  primaryClass = {cs},
  author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
  date = {2015-07-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Shi_et_al_2015_An_End-to-End_Trainable_Neural_Network_for_Image-based_Sequence_Recognition_and.pdf:/Users/jschmolling/Dropbox/ebooks/Shi_et_al_2015_An_End-to-End_Trainable_Neural_Network_for_Image-based_Sequence_Recognition_and.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/8GA7U8WG/1507.html:text/html},
  groups = {ocr}
}

@article{jaderberg_synthetic_2014,
  title = {Synthetic {{Data}} and {{Artificial Neural Networks}} for {{Natural Scene Text Recognition}}},
  url = {http://arxiv.org/abs/1406.2227},
  abstract = {In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.},
  timestamp = {2017-06-07T08:53:17Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2227},
  primaryClass = {cs},
  author = {Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-06-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Jaderberg_et_al_2014_Synthetic_Data_and_Artificial_Neural_Networks_for_Natural_Scene_Text_Recognition.pdf:/Users/jschmolling/Dropbox/ebooks/Jaderberg_et_al_2014_Synthetic_Data_and_Artificial_Neural_Networks_for_Natural_Scene_Text_Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/EQ4I8XVD/1406.html:text/html},
  groups = {ocr}
}

@inproceedings{yang_hierarchical_2016,
  title = {Hierarchical Attention Networks for Document Classification},
  url = {http://www.aclweb.org/anthology/N16-1174},
  timestamp = {2017-06-07T09:25:31Z},
  booktitle = {Proceedings of {{NAACL}}-{{HLT}}},
  author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  date = {2016},
  pages = {1480--1489},
  file = {Yang_et_al_2016_Hierarchical_attention_networks_for_document_classification.pdf:/Users/jschmolling/Dropbox/ebooks/Yang_et_al_2016_Hierarchical_attention_networks_for_document_classification.pdf:application/pdf}
}

@article{santoro_simple_2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  url = {http://arxiv.org/abs/1706.01427},
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  timestamp = {2017-07-05T14:19:06Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.01427},
  primaryClass = {cs},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  date = {2017-06-05},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning},
  file = {Santoro_et_al_2017_A_simple_neural_network_module_for_relational_reasoning.pdf:/Users/jschmolling/Dropbox/ebooks/Santoro_et_al_2017_A_simple_neural_network_module_for_relational_reasoning.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/64S6X7BT/1706.html:text/html}
}

@book{__1967,
  title = {Советское фото, вып. 05, 1967 г.},
  url = {http://archive.org/details/sovphoto_v1_1967_05},
  abstract = {Советское фото
Журнал фотолюбительства и фоторепортажа
Sovetskoe Photo magazine
 выпуск/issue 05, год/year 1967},
  timestamp = {2017-07-26T08:45:15Z},
  langid = {russian},
  author = {{Огонёк}},
  date = {1967},
  keywords = {landscape photography,magazine,nature,photography,Sovetskoe Foto,Soviet Photo,street photography,ussr,журнал,природа,Советское фото,СССР,фотография}
}

@online{_my_????,
  title = {My {{Neural Network}} Isn't Working! {{What}} Should {{I}} Do?},
  url = {http://theorangeduck.com/page/neural-network-not-working},
  timestamp = {2017-09-06T23:10:05Z},
  urldate = {2017-09-06},
  file = {My Neural Network isn't working! What should I do?:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/Z46U7YZF/neural-network-not-working.html:text/html},
  groups = {ml-allgemein}
}

@article{hinton_reducing_2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  volume = {313},
  url = {http://science.sciencemag.org/content/313/5786/504.short},
  timestamp = {2017-10-11T07:45:32Z},
  number = {5786},
  journaltitle = {science},
  author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
  date = {2006},
  pages = {504--507},
  file = {Hinton_Salakhutdinov_2006_Reducing_the_dimensionality_of_data_with_neural_networks.pdf:/Users/jschmolling/Dropbox/ebooks/Hinton_Salakhutdinov_2006_Reducing_the_dimensionality_of_data_with_neural_networks.pdf:application/pdf;Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/A6CTKW9S/504.html:text/html},
  groups = {unsupervised}
}

@article{doersch_unsupervised_2015,
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  url = {http://arxiv.org/abs/1505.05192},
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
  timestamp = {2017-10-22T15:20:57Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05192},
  primaryClass = {cs},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  date = {2015-05-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Doersch_et_al_2015_Unsupervised_Visual_Representation_Learning_by_Context_Prediction.pdf:/Users/jschmolling/Dropbox/ebooks/Doersch_et_al_2015_Unsupervised_Visual_Representation_Learning_by_Context_Prediction.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/A5KU63WN/1505.html:text/html},
  groups = {unsupervised}
}

@article{sun_revisiting_2017,
  title = {Revisiting {{Unreasonable Effectiveness}} of {{Data}} in {{Deep Learning Era}}},
  url = {http://arxiv.org/abs/1707.02968},
  abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
  timestamp = {2017-10-22T15:22:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.02968},
  primaryClass = {cs},
  author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  date = {2017-07-10},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {Sun_et_al_2017_Revisiting_Unreasonable_Effectiveness_of_Data_in_Deep_Learning_Era.pdf:/Users/jschmolling/Dropbox/ebooks/Sun_et_al_2017_Revisiting_Unreasonable_Effectiveness_of_Data_in_Deep_Learning_Era.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/2T3QUK87/1707.html:text/html},
  groups = {maybe}
}

@inproceedings{joshi_segmentation_2016,
  title = {A Segmentation Approach Based on Structured Learning for Recognition Preprocessing},
  doi = {10.1109/ICACDOT.2016.7877724},
  abstract = {Segmentation of manually written record photos into content material traces and words is a crucial undertaking for optical character acknowledgment. However, for the reason that elements of written by hand record are abnormal and various depending at the character, so far it is considered as a challenging issue. A good way to deal with the hassle, we formulate the word segmentation problem as a binary quadratic undertaking problem that considers pair-wise relationships among the crevices and in addition the probabilities of individual holes. No matter the fact that sever a parameters are covered in this detailing, we gauge all parameters taking into account the based Naive Bayes classifier so that the proposed technique features admirably paying little admire to composing styles and composed dialects without patron characterized parameters. Test effects on ICDAR 2009/2013 penmanship division data-set exhibit that proposed method accomplishes the high-quality in magnificence execution on Latin-primarily based and Indian languages.},
  eventtitle = {2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)},
  timestamp = {2017-11-04T14:49:45Z},
  booktitle = {2016 {{International Conference}} on {{Automatic Control}} and {{Dynamic Optimization Techniques}} ({{ICACDOT}})},
  author = {Joshi, A. and Bharadwaj, D.},
  date = {2016-09},
  pages = {935--939},
  keywords = {Bayes methods,binary quadratic,Classification algorithms,Computers,Dictionaries,document image processing,electronic mail,handwritten character recognition,Handwritten documents,image classification,image segmentation,Indian language,Latin language,learning (artificial intelligence),naive Bayes classifier,natural language processing,optical character acknowledgment,optical character recognition,Optimization,pair-wise relationships,recognition preprocessing,structured learning,text detection,Transforms,word segmentation,word segmentation problem,written record photos},
  file = {IEEE Xplore Abstract Record:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/CA9EIFK2/7877724.html:text/html},
  groups = {discovery}
}

@inproceedings{al-barhamtoshy_universal_2016,
  title = {Universal Metadata Repository for Document Analysis and Recognition},
  doi = {10.1109/AICCSA.2016.7945622},
  abstract = {Document Analysis and Recognition (DAR) has two main objectives, first the analysis of the physical structure of the input image of the document, which should lead to the correct identification of the corresponding different homogeneous components and their boundaries in terms of XY coordinates. Second, each of these homogeneous components should be recognized in such a way that, if it is a text image, consequently this image should be recognized and translated into an intelligible text. DAR remains one of the most challenging topics in pattern recognition. Indeed, despite the diversity of the proposed approaches, techniques and methods, results remain very weak and away from expectations especially for several categories of documents such as complex, low quality, handwritten and historical documents. The complex structure and/or morphology of such documents are behind the weakness of results of these proposed approaches, techniques and methods. One of the challenging problems related to this topic is the creation of standard datasets that can be used by all stakeholders of this topic such as system developers, expert evaluators, and users. In addition, another challenging problem is how one could take advantages of all existing datasets that unfortunately are dispersed around the world without knowing, most of the times, any information about their locations and the way to reach them. As an attempt to solve the two mentioned above problems, we propose in this paper a Universal Datasets Repository for Document Analysis and Recognition (UMDAR) that has, in fact, a twofold advantage. First, it can help dataset creators to standardize their datasets and making them accessible to the research community once published on the proposed repository. Second, it can be used as a central which bridges in a smart manner between datasets and all DAR stakeholders.},
  eventtitle = {2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)},
  timestamp = {2017-11-04T14:51:02Z},
  booktitle = {2016 {{IEEE}}/{{ACS}} 13th {{International Conference}} of {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  author = {Al-Barhamtoshy, H. and Khemakhem, M. and Jambi, K. and Essa, F. and Fattouh, A. and Al-Ghamdi, A.},
  date = {2016-11},
  pages = {1--6},
  keywords = {Business,Databases,dataset,document analysis and recognition,document image,document image processing,image recognition,image segmentation,intelligible text,meta data,Metadata,pattern recognition,repositor,Standards,Text analysis,text image,Training,UMDAR,Universal Datasets Repository for Document Analysis and Recognition,universal metadata repository},
  file = {IEEE Xplore Abstract Record:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/N5PMN25U/7945622.html:text/html},
  groups = {discovery}
}

@article{chen_convolutional_2017,
  title = {Convolutional {{Neural Networks}} for {{Page Segmentation}} of {{Historical Document Images}}},
  url = {http://arxiv.org/abs/1704.01474},
  abstract = {This paper presents a Convolutional Neural Network (CNN) based page segmentation method for handwritten historical document images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.},
  timestamp = {2017-11-04T15:10:51Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.01474},
  primaryClass = {cs, stat},
  author = {Chen, Kai and Seuret, Mathias},
  date = {2017-04-05},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  file = {Chen_Seuret_2017_Convolutional_Neural_Networks_for_Page_Segmentation_of_Historical_Document.pdf:/Users/jschmolling/Dropbox/ebooks/Chen_Seuret_2017_Convolutional_Neural_Networks_for_Page_Segmentation_of_Historical_Document.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/83RMJ6F6/1704.html:text/html},
  groups = {books,discovery}
}

@inproceedings{clausner_document_2014,
  location = {{New York, NY, USA}},
  title = {Document {{Representation Refinement}} for {{Precise Region Description}}},
  isbn = {978-1-4503-2588-2},
  url = {http://doi.acm.org/10.1145/2595188.2595198},
  doi = {10.1145/2595188.2595198},
  abstract = {Precise description of layout entities (content regions on a page) is crucial for all but the most trivial document analysis and recognition applications. The output of layout analysis methods and state-of-the-art OCR systems varies significantly, from bounding boxes (e.g. Tesseract) to stacks of text line rectangles (e.g. ABBYY FineReader). There is a clear need for a consistent and accurate representation of regions (e.g. text paragraphs, graphics entities etc.) for further processing, correction and performance evaluation (comparison of segmentation results with ground truth regions). This paper describes a method for refinement of document representations by fitting polygons around lower-level layout objects (such as text lines, words and glyphs) in a systematic way that reconstructs region outlines and preserves the fine details of complex layouts. Experimental results on a standard dataset demonstrate the validity and usefulness of the proposed approach.},
  timestamp = {2017-11-04T16:48:49Z},
  booktitle = {Proceedings of the {{First International Conference}} on {{Digital Access}} to {{Textual Cultural Heritage}}},
  series = {DATeCH '14},
  publisher = {{ACM}},
  author = {Clausner, Christian and Pletschacher, Stefan and Antonacopoulos, Apostolos},
  date = {2014},
  pages = {9--13},
  keywords = {document image analysis,polygonal fitting,segmentation},
  file = {Clausner_et_al_2014_Document_Representation_Refinement_for_Precise_Region_Description.pdf:/Users/jschmolling/Dropbox/ebooks/Clausner_et_al_2014_Document_Representation_Refinement_for_Precise_Region_Description.pdf:application/pdf},
  groups = {discovery}
}

@article{gruning_read-bad:_2017,
  title = {{{READ}}-{{BAD}}: {{A New Dataset}} and {{Evaluation Scheme}} for {{Baseline Detection}} in {{Archival Documents}}},
  url = {http://arxiv.org/abs/1705.03311},
  shorttitle = {{{READ}}-{{BAD}}},
  abstract = {Text line detection is crucial for any application associated with Automatic Text Recognition or Keyword Spotting. Modern algorithms perform good on well-established datasets since they either comprise clean data or simple/homogeneous page layouts. We have collected and annotated 2036 archival document images from different locations and time periods. The dataset contains varying page layouts and degradations that challenge text line segmentation methods. Well established text line segmentation evaluation schemes such as the Detection Rate or Recognition Accuracy demand for binarized data that is annotated on a pixel level. Producing groundtruth by these means is laborious and not needed to determine a method's quality. In this paper we propose a new evaluation scheme that is based on baselines. The proposed scheme has no need for binarization, it can handle skewed and rotated text lines and its results correlate with Handwritten Text Recognition accuracy. The ICDAR 2017 Competition on Baseline Detection and the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts make use of this evaluation scheme.},
  timestamp = {2017-11-04T16:50:24Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.03311},
  primaryClass = {cs},
  author = {Grüning, Tobias and Labahn, Roger and Diem, Markus and Kleber, Florian and Fiel, Stefan},
  date = {2017-05-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Grüning_et_al_2017_READ-BAD.pdf:/Users/jschmolling/Dropbox/ebooks/Grüning_et_al_2017_READ-BAD.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/F4SID7WE/1705.html:text/html},
  groups = {evaluation/datasets}
}

@online{_saint_????,
  title = {Saint {{Gall Database}} — {{Computer Vision}} and {{Artificial Intelligence}}},
  url = {http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/saint-gall-database},
  timestamp = {2017-11-04T16:51:46Z},
  urldate = {2017-11-04},
  groups = {datasets/evaluation}
}

@online{_parzival_????,
  title = {Parzival {{Database}} — {{Computer Vision}} and {{Artificial Intelligence}}},
  url = {http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/parzival-database},
  timestamp = {2017-11-04T16:53:48Z},
  urldate = {2017-11-04},
  groups = {datasets/evaluation}
}

@article{vats_automatic_2017,
  title = {Automatic {{Document Image Binarization}} Using {{Bayesian Optimization}}},
  url = {http://arxiv.org/abs/1709.01782},
  abstract = {Document image binarization is often a challenging task due to various forms of degradation. Although there exist several binarization techniques in literature, the binarized image is typically sensitive to control parameter settings of the employed technique. This paper presents an automatic document image binarization algorithm to segment the text from heavily degraded document images. The proposed technique uses a two band-pass filtering approach for background noise removal, and Bayesian optimization for automatic hyperparameter selection for optimal results. The effectiveness of the proposed binarization technique is empirically demonstrated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets.},
  timestamp = {2017-11-04T16:59:33Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01782},
  primaryClass = {cs},
  author = {Vats, Ekta and Hast, Anders and Singh, Prashant},
  date = {2017-09-06},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {Vats_et_al_2017_Automatic_Document_Image_Binarization_using_Bayesian_Optimization.pdf:/Users/jschmolling/Dropbox/ebooks/Vats_et_al_2017_Automatic_Document_Image_Binarization_using_Bayesian_Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/6NNKJR4I/1709.html:text/html},
  groups = {evaluation/exp}
}

@online{_icdar_????,
  title = {{{ICDAR}} 2017 {{Document Image Binarization Competition}} ({{DIBCO}} 2017)},
  url = {https://vc.ee.duth.gr/dibco2017/},
  timestamp = {2017-11-04T16:59:51Z},
  urldate = {2017-11-04},
  groups = {evaluation/exp}
}

@incollection{marinai_introduction_2008,
  title = {Introduction to {{Document Analysis}} and {{Recognition}}},
  isbn = {978-3-540-76279-9 978-3-540-76280-5},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-76280-5_1},
  abstract = {Document Analysis and Recognition (DAR) aims at the automatic extraction of information presented on paper and initially addressed to human comprehension. The desired output of DAR systems is usually in a suitable symbolic representation that can subsequently be processed by computers.},
  timestamp = {2017-11-04T17:09:45Z},
  langid = {english},
  booktitle = {Machine {{Learning}} in {{Document Analysis}} and {{Recognition}}},
  series = {Studies in Computational Intelligence},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Marinai, Simone},
  urldate = {2017-11-04},
  date = {2008},
  pages = {1--20},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/PNTJRJ78/978-3-540-76280-5_1.html:text/html},
  groups = {Themegebiete,Themegebiete},
  doi = {10.1007/978-3-540-76280-5_1}
}

@book{_handbook_????,
  title = {Handbook of {{Document Image Processing}} and {{Recognition}} | {{David Doermann}} | {{Springer}}},
  url = {//www.springer.com/de/book/9780857298584},
  abstract = {The Handbook of Document Image Processing and Recognition is a comprehensive resource on the latest methods and techniques in document image processing...},
  timestamp = {2017-11-04T17:12:19Z},
  urldate = {2017-11-04},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/KGARB85E/9780857298584.html:text/html},
  groups = {Themegebiete,Themegebiete}
}

@inproceedings{sanchez_icdar_2015,
  title = {{{ICDAR}} 2015 Competition {{HTRtS}}: {{Handwritten Text Recognition}} on the {{tranScriptorium}} Dataset},
  doi = {10.1109/ICDAR.2015.7333944},
  shorttitle = {{{ICDAR}} 2015 Competition {{HTRtS}}},
  abstract = {This paper describes the second edition of the Handwritten Text Recognition (HTR) contest on the tranScriptorium datasets that has been held in the context of the International Conference on Document Analysis and Recognition 2015. Two tracks with different conditions on the use of training data were proposed. Nine research groups registered in the contest but finally three research submitted results. The handwritten images for this contest were drawn from the English “Bentham collection” dataset used in the tranScriptorium project. A small subset of this collection has been chosen for the present HTR competition. The selected subset has been written by several hands and entails significant variabilities and difficulties regarding the quality of text images, writing styles and crossed-out text. This contest is clearly more difficult than the the first edition both for training and for testing. A portion of the training dataset and the full test dataset were provided in the form of carefully segmented line images, along with the corresponding transcripts. Another portion of the training dataset was provided as raw images and their corresponding transcripts at region level. The three participants achieved good results, with transcription word error rates ranging from 31\% down to 44\%.},
  eventtitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  timestamp = {2017-11-04T17:17:09Z},
  booktitle = {2015 13th {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Sánchez, J. A. and Toselli, A. H. and Romero, V. and Vidal, E.},
  date = {2015-08},
  pages = {1166--1170},
  keywords = {Bentham collection dataset,document image processing,handwriting recognition,handwritten text recognition,Hidden Markov models,Historical Documents,HTR,ICDAR 2015 competition HTRtS,image recognition,image segmentation,international conference on document analysis and recognition,natural language processing,Text analysis,text image quality,transcriptorium dataset},
  file = {Sánchez_et_al_2015_ICDAR_2015_competition_HTRtS.pdf:/Users/jschmolling/Dropbox/ebooks/Sánchez_et_al_2015_ICDAR_2015_competition_HTRtS.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/PZXV3BZR/7333944.html:text/html},
  groups = {evaluation/exp}
}

@inproceedings{mcconnaughey_labeled_2017,
  title = {The {{Labeled Segmentation}} of {{Printed Books}}},
  timestamp = {2017-11-07T10:05:15Z},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {McConnaughey, Lara and Dai, Jennifer and Bamman, David},
  date = {2017},
  pages = {748--758},
  file = {McConnaughey_et_al_2017_The_Labeled_Segmentation_of_Printed_Books.pdf:/Users/jschmolling/Dropbox/ebooks/McConnaughey_et_al_2017_The_Labeled_Segmentation_of_Printed_Books.pdf:application/pdf;Fulltext:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/J7TM9KT7/McConnaughey et al. - 2017 - The Labeled Segmentation of Printed Books.pdf:application/pdf},
  groups = {books}
}

@inproceedings{clausner_document_2014-1,
  location = {{New York, NY, USA}},
  title = {Document {{Representation Refinement}} for {{Precise Region Description}}},
  isbn = {978-1-4503-2588-2},
  url = {http://doi.acm.org/10.1145/2595188.2595198},
  doi = {10.1145/2595188.2595198},
  abstract = {Precise description of layout entities (content regions on a page) is crucial for all but the most trivial document analysis and recognition applications. The output of layout analysis methods and state-of-the-art OCR systems varies significantly, from bounding boxes (e.g. Tesseract) to stacks of text line rectangles (e.g. ABBYY FineReader). There is a clear need for a consistent and accurate representation of regions (e.g. text paragraphs, graphics entities etc.) for further processing, correction and performance evaluation (comparison of segmentation results with ground truth regions). This paper describes a method for refinement of document representations by fitting polygons around lower-level layout objects (such as text lines, words and glyphs) in a systematic way that reconstructs region outlines and preserves the fine details of complex layouts. Experimental results on a standard dataset demonstrate the validity and usefulness of the proposed approach.},
  timestamp = {2017-11-07T10:10:44Z},
  booktitle = {Proceedings of the {{First International Conference}} on {{Digital Access}} to {{Textual Cultural Heritage}}},
  series = {DATeCH '14},
  publisher = {{ACM}},
  author = {Clausner, Christian and Pletschacher, Stefan and Antonacopoulos, Apostolos},
  date = {2014},
  pages = {9--13},
  keywords = {document image analysis,polygonal fitting,segmentation},
  groups = {books}
}

@inproceedings{henning_estimating_2017,
  location = {{New York, NY, USA}},
  title = {Estimating the {{Information Gap Between Textual}} and {{Visual Representations}}},
  isbn = {978-1-4503-4701-3},
  url = {http://doi.acm.org/10.1145/3078971.3078991},
  doi = {10.1145/3078971.3078991},
  abstract = {Photos, drawings, figures, etc. supplement textual information in various kinds of media, for example, in web news or scientific publications. In this respect, the intended effect of an image can be quite different, e.g., providing additional information, focusing on certain details of surrounding text, or simply being a general illustration of a topic. As a consequence, the semantic correlation between information of different modalities can vary noticeably, too. Moreover, cross-modal interrelations are often hard to describe in a precise way. The variety of possible interrelations of textual and graphical information and the question, how they can be described and automatically estimated have not been addressed yet by previous work. In this paper, we present several contributions to close this gap. First, we introduce two measures to describe cross-modal interrelations: cross-modal mutual information (CMI) and semantic correlation (SC). Second, a novel approach relying on deep learning is suggested to estimate CMI and SC of textual and visual information. Third, three diverse datasets are leveraged to learn an appropriate deep neural network model for the demanding task. The system has been evaluated on a challenging test set and the experimental results demonstrate the feasibility of the approach.},
  timestamp = {2017-11-08T15:48:45Z},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{International Conference}} on {{Multimedia Retrieval}}},
  series = {ICMR '17},
  publisher = {{ACM}},
  author = {Henning, Christian Andreas and Ewerth, Ralph},
  date = {2017},
  pages = {14--22},
  keywords = {deep learning,multimodal embeddings,text-image relations},
  file = {Henning_Ewerth_2017_Estimating_the_Information_Gap_Between_Textual_and_Visual_Representations.pdf:/Users/jschmolling/Dropbox/ebooks/Henning_Ewerth_2017_Estimating_the_Information_Gap_Between_Textual_and_Visual_Representations.pdf:application/pdf},
  groups = {books}
}

@inproceedings{rubinstein_unsupervised_2013,
  title = {Unsupervised Joint Object Discovery and Segmentation in Internet Images},
  timestamp = {2017-11-14T12:56:58Z},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Rubinstein, Michael and Joulin, Armand and Kopf, Johannes and Liu, Ce},
  date = {2013},
  pages = {1939--1946},
  file = {Rubinstein_et_al_2013_Unsupervised_joint_object_discovery_and_segmentation_in_internet_images.pdf:/Users/jschmolling/Dropbox/ebooks/Rubinstein_et_al_2013_Unsupervised_joint_object_discovery_and_segmentation_in_internet_images.pdf:application/pdf;Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/54JTGWIS/Rubinstein_Unsupervised_Joint_Object_2013_CVPR_paper.html:text/html},
  groups = {unsupervised}
}

@article{doersch_what_2012,
  title = {What Makes Paris Look like Paris?},
  volume = {31},
  timestamp = {2017-11-15T13:06:30Z},
  number = {4},
  journaltitle = {ACM Transactions on Graphics},
  author = {Doersch, Carl and Singh, Saurabh and Gupta, Abhinav and Sivic, Josef and Efros, Alexei},
  date = {2012},
  file = {Doersch_et_al_2012_What_makes_paris_look_like_paris.pdf:/Users/jschmolling/Dropbox/ebooks/Doersch_et_al_2012_What_makes_paris_look_like_paris.pdf:application/pdf;Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DP6BMC32/hal-01053876.html:text/html},
  groups = {unsupervised}
}

@article{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  url = {http://arxiv.org/abs/1502.03167},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  timestamp = {2017-11-15T13:12:53Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-02-10},
  keywords = {Computer Science - Learning},
  file = {Ioffe_Szegedy_2015_Batch_Normalization.pdf:/Users/jschmolling/Dropbox/ebooks/Ioffe_Szegedy_2015_Batch_Normalization.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/7KSTVU9X/1502.html:text/html},
  groups = {techniques}
}

@inproceedings{papadopoulos_impact_2013,
  title = {The {{IMPACT}} Dataset of Historical Document Images},
  isbn = {978-1-4503-2115-0},
  url = {http://dl.acm.org/citation.cfm?doid=2501115.2501130},
  doi = {10.1145/2501115.2501130},
  timestamp = {2017-11-23T02:18:56Z},
  langid = {english},
  publisher = {{ACM Press}},
  author = {Papadopoulos, Christos and Pletschacher, Stefan and Clausner, Christian and Antonacopoulos, Apostolos},
  urldate = {2017-11-23},
  date = {2013},
  pages = {123},
  file = {Papadopoulos_et_al_2013_The_IMPACT_dataset_of_historical_document_images.pdf:/Users/jschmolling/Dropbox/ebooks/Papadopoulos_et_al_2013_The_IMPACT_dataset_of_historical_document_images.pdf:application/pdf},
  groups = {evaluation/datasets}
}

@article{chen_deeplab:_2016,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  url = {http://arxiv.org/abs/1606.00915},
  shorttitle = {{{DeepLab}}},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  timestamp = {2017-11-23T02:20:23Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00915},
  primaryClass = {cs},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  date = {2016-06-02},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Chen_et_al_2016_DeepLab.pdf:/Users/jschmolling/Dropbox/ebooks/Chen_et_al_2016_DeepLab.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/QGDESVWN/1606.html:text/html},
  groups = {segmentation}
}

@article{zhang_understanding_2016,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  url = {http://arxiv.org/abs/1611.03530},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  timestamp = {2017-12-01T02:02:05Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03530},
  primaryClass = {cs},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  urldate = {2017-12-01},
  date = {2016-11-10},
  keywords = {Computer Science - Learning},
  file = {arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/6SRD5HI4/1611.html:text/html},
  groups = {ml-allgemein}
}

@comment{jabref-meta: databaseType:biblatex;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:books\;0\;;
1 ExplicitGroup:discovery\;0\;;
1 ExplicitGroup:doc-clustering\;0\;;
1 ExplicitGroup:doc-datasets\;0\;;
1 ExplicitGroup:Domain\;0\;;
2 ExplicitGroup:buchbeispiele\;0\;;
2 ExplicitGroup:Datenquellen\;0\;;
2 ExplicitGroup:Themegebiete\;0\;;
1 ExplicitGroup:buchbeispiele\;0\;;
1 ExplicitGroup:Datenquellen\;0\;;
1 ExplicitGroup:Themegebiete\;0\;;
1 ExplicitGroup:evaluation/datasets\;0\;;
1 ExplicitGroup:evaluation/exp\;0\;;
1 ExplicitGroup:Gliederung\;0\;;
1 ExplicitGroup:htw-masterarbeiten\;0\;;
1 ExplicitGroup:maybe\;0\;;
1 ExplicitGroup:ml-allgemein\;0\;;
1 ExplicitGroup:nn-feature-clustering\;0\;;
1 ExplicitGroup:ocr\;0\;;
1 ExplicitGroup:region-proposal\;0\;;
1 ExplicitGroup:segmentation\;0\;;
1 ExplicitGroup:Software\;0\;;
1 ExplicitGroup:techniques\;0\;;
1 ExplicitGroup:unsupervised\;0\;;
1 ExplicitGroup:words\;0\;;
}

