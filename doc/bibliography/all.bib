
@inproceedings{LeBuildinghighlevelfeatures2013,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6639343},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2013 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  urldate = {2016-04-20},
  date = {2013},
  pages = {8595--8598},
  author = {Le, Quoc V.},
  file = {/Users/jschmolling/Dropbox/ebooks/Le_2013_Building_high-level_features_using_large_scale_unsupervised_learning.pdf}
}

@online{facebookresearchfastTextLibrary,
  title = {Facebookresearch/{{fastText}}: {{Library}} for Fast Text Representation and Classification.},
  url = {https://github.com/facebookresearch/fastText},
  urldate = {2017-02-14},
  file = {/home/jakob/Zotero/storage/GPRTMWNR/fastText.html}
}

@online{gensimtopicmodelling,
  title = {Gensim: Topic Modelling for Humans},
  url = {https://radimrehurek.com/gensim/index.html},
  shorttitle = {Gensim},
  abstract = {Efficient topic modelling in Python},
  urldate = {2017-02-14},
  file = {/home/jakob/Zotero/storage/IZZNEN2N/index.html}
}

@article{JoulinBagTricksEfficient2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.01759},
  primaryClass = {cs},
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  url = {http://arxiv.org/abs/1607.01759},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore\textasciitilde{}CPU, and classify half a million sentences among\textasciitilde{}312K classes in less than a minute.},
  urldate = {2017-02-14},
  date = {2016-07-06},
  keywords = {Computer Science - Computation and Language},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  file = {/Users/jschmolling/Dropbox/ebooks/Joulin_et_al_2016_Bag_of_Tricks_for_Efficient_Text_Classification.pdf;/home/jakob/Zotero/storage/QMXKCSEM/1607.html}
}

@article{BojanowskiEnrichingWordVectors2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.04606},
  primaryClass = {cs},
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  url = {http://arxiv.org/abs/1607.04606},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpus quickly. We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.},
  urldate = {2017-02-14},
  date = {2016-07-15},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  file = {/Users/jschmolling/Dropbox/ebooks/Bojanowski_et_al_2016_Enriching_Word_Vectors_with_Subword_Information.pdf;/home/jakob/Zotero/storage/333UPX68/1607.html},
  annotation = {Comment: Submitted to EMNLP 2016}
}

@article{GirshickRichfeaturehierarchies2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.2524},
  primaryClass = {cs},
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  url = {http://arxiv.org/abs/1311.2524},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde{}rbg/rcnn.},
  urldate = {2016-11-23},
  date = {2013-11-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  file = {/Users/jschmolling/Dropbox/ebooks/Girshick_et_al_2013_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation.pdf;/home/jakob/Zotero/storage/PDCN5GNP/1311.html},
  annotation = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)}
}

@online{kyamagujssegmentannotator,
  title = {Kyamagu/Js-Segment-Annotator},
  url = {https://github.com/kyamagu/js-segment-annotator},
  abstract = {js-segment-annotator - Javascript image annotation tool based on image segmentation.},
  journaltitle = {GitHub},
  urldate = {2016-08-10},
  file = {/home/jakob/Zotero/storage/J5UTA9HU/js-segment-annotator.html}
}

@article{DundarConvolutionalClusteringUnsupervised2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06241},
  primaryClass = {cs},
  title = {Convolutional {{Clustering}} for {{Unsupervised Learning}}},
  url = {http://arxiv.org/abs/1511.06241},
  abstract = {The task of labeling data for training deep neural networks is daunting and tedious, requiring millions of labels to achieve the current state-of-the-art results. Such reliance on large amounts of labeled data can be relaxed by exploiting hierarchical features via unsupervised learning techniques. In this work, we propose to train a deep convolutional network based on an enhanced version of the k-means clustering algorithm, which reduces the number of correlated parameters in the form of similar filters, and thus increases test categorization accuracy. We call our algorithm convolutional k-means clustering. We further show that learning the connection between the layers of a deep convolutional neural network improves its ability to be trained on a smaller amount of labeled data. Our experiments show that the proposed algorithm outperforms other techniques that learn filters unsupervised. Specifically, we obtained a test accuracy of 74.1\% on STL-10 and a test error of 0.5\% on MNIST.},
  urldate = {2016-08-10},
  date = {2015-11-19},
  keywords = {Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition},
  author = {Dundar, Aysegul and Jin, Jonghoon and Culurciello, Eugenio},
  file = {/Users/jschmolling/Dropbox/ebooks/Dundar_et_al_2015_Convolutional_Clustering_for_Unsupervised_Learning.pdf;/home/jakob/Zotero/storage/34MPWQDG/1511.html},
  annotation = {Comment: 11 pages}
}

@article{ZhaoStackedWhatWhereAutoencoders2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02351},
  primaryClass = {cs, stat},
  title = {Stacked {{What}}-{{Where Auto}}-Encoders},
  url = {http://arxiv.org/abs/1506.02351},
  abstract = {We present a novel architecture, the "stacked what-where auto-encoders" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the "what" which are fed to the next layer, and its complementary variable "where" that are fed to the corresponding layer in the generative decoder.},
  urldate = {2016-08-10},
  date = {2015-06-08},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  author = {Zhao, Junbo and Mathieu, Michael and Goroshin, Ross and LeCun, Yann},
  file = {/Users/jschmolling/Dropbox/ebooks/Zhao_et_al_2015_Stacked_What-Where_Auto-encoders.pdf;/home/jakob/Zotero/storage/5AV6Z67U/1506.html},
  annotation = {Comment: Workshop track - ICLR 2016}
}

@article{AchantaSLICSuperpixelsCompared2012,
  title = {{{SLIC Superpixels Compared}} to {{State}}-of-the-{{Art Superpixel Methods}}},
  volume = {34},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2012.120},
  number = {11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2012-11},
  pages = {2274--2282},
  author = {Achanta, R. and Shaji, A. and Smith, K. and Lucchi, A. and Fua, P. and Süsstrunk, Sabine},
  file = {/Users/jschmolling/Dropbox/ebooks/Achanta_et_al_2012_SLIC_Superpixels_Compared_to_State-of-the-Art_Superpixel_Methods.pdf}
}

@article{MuhlbergerHandwrittenTextRecognition,
  title = {Handwritten {{Text Recognition}} ({{HTR}}) of {{Historical Documents}} as a {{Shared Task}} for {{Archivists}}, {{Computer Scientists}} and {{Humanities Scholars}}. {{The Model}} of a {{Transcription}} \& {{Recognition Platform}} ({{TRP}})},
  url = {http://www.academia.edu/download/34967812/muehlberger_histoinformatics-2.pdf},
  urldate = {2016-07-28},
  author = {Mühlberger, Günter and Colutto, Sebastian and Kahle, Philip}
}

@article{RedmonYouOnlyLook2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02640},
  primaryClass = {cs},
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  url = {http://arxiv.org/abs/1506.02640},
  shorttitle = {You {{Only Look Once}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  urldate = {2016-07-26},
  date = {2015-06-08},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  file = {/Users/jschmolling/Dropbox/ebooks/Redmon_et_al_2015_You_Only_Look_Once.pdf}
}

@article{HarleyEvaluationDeepConvolutional2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.07058},
  primaryClass = {cs},
  title = {Evaluation of {{Deep Convolutional Nets}} for {{Document Image Classification}} and {{Retrieval}}},
  url = {http://arxiv.org/abs/1502.07058},
  abstract = {This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.},
  urldate = {2016-07-25},
  date = {2015-02-25},
  keywords = {Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Information Retrieval},
  author = {Harley, Adam W. and Ufkes, Alex and Derpanis, Konstantinos G.},
  file = {/Users/jschmolling/Dropbox/ebooks/Harley_et_al_2015_Evaluation_of_Deep_Convolutional_Nets_for_Document_Image_Classification_and.pdf;/home/jakob/Zotero/storage/EF48WJAS/1502.html}
}

@inreference{DigitalHumanities2016,
  langid = {german},
  title = {Digital Humanities},
  url = {https://de.wikipedia.org/w/index.php?title=Digital_Humanities&oldid=156118661},
  abstract = {Das Fach Digital Humanities (deutsch: „digitale Geisteswissenschaften“) umfasst die Anwendung von computergestützten Verfahren und die systematische Verwendung von digitalen Ressourcen in den Geistes- und Kulturwissenschaften. Es handelt sich um ein interdisziplinär ausgerichtetes Fach, dessen Vertreter sowohl durch eine traditionelle Ausbildung in den Geistes- und Kulturwissenschaften ausgewiesen sind wie durch ihre Vertrautheit mit einer Reihe von einschlägigen Konzepten, Verfahren und Standards der Informatik. In Deutschland sind das insbesondere Forscher der Computerphilologie, der Historischen Fachinformatik, der Informationswissenschaft und der Computerlinguistik. Typische Arbeits- und Forschungsfelder sind z.B. digitale Editionen, quantitative Textanalyse, Visualisierung komplexer Datenstrukturen oder die Theorie digitaler Medien.},
  booktitle = {Wikipedia},
  urldate = {2016-07-25},
  date = {2016-07-13T20:32:31Z},
  file = {/home/jakob/Zotero/storage/GSQ4E2QK/index.html},
  note = {Page Version ID: 156118661}
}

@article{RenFasterRCNNRealTime2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  url = {http://arxiv.org/abs/1506.01497},
  shorttitle = {Faster {{R}}-{{CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  urldate = {2016-07-25},
  date = {2015-06-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  file = {/Users/jschmolling/Dropbox/ebooks/Ren_et_al_2015_Faster_R-CNN.pdf;/home/jakob/Zotero/storage/8KW3TTPZ/1506.html},
  annotation = {Comment: Extended tech report}
}

@article{RazavianCNNFeaturesofftheshelf2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.6382},
  primaryClass = {cs},
  title = {{{CNN Features}} Off-the-Shelf: An {{Astounding Baseline}} for {{Recognition}}},
  url = {http://arxiv.org/abs/1403.6382},
  shorttitle = {{{CNN Features}} Off-the-Shelf},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the $\backslash$overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the $\backslash$overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the $\backslash$overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  urldate = {2016-07-24},
  date = {2014-03-23},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  file = {/Users/jschmolling/Dropbox/ebooks/Razavian_et_al_2014_CNN_Features_off-the-shelf.pdf},
  annotation = {Comment: version 3 revisions: 1)Added results using feature processing and data augmentation 2)Referring to most recent efforts of using CNN for different visual recognition tasks 3) updated text/caption}
}

@article{GirshickRegionbasedconvolutionalnetworks2016,
  title = {Region-Based Convolutional Networks for Accurate Object Detection and Segmentation},
  volume = {38},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7112511},
  number = {1},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  urldate = {2016-07-24},
  date = {2016},
  pages = {142--158},
  keywords = {segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  file = {/Users/jschmolling/Dropbox/ebooks/Girshick_et_al_2016_Region-based_convolutional_networks_for_accurate_object_detection_and.pdf}
}

@book{CloughBestBothWorlds2013,
  langid = {english},
  title = {Best of {{Both Worlds}}: {{Museums}}, {{Libraries}}, and {{Archives}} in the {{Digital Age}}},
  shorttitle = {Best of {{Both Worlds}}},
  abstract = {Wayne Clough, Secretary of the Smithsonian Institution, asks museums a fundamental question: "How can we prepare ourselves to reach the generation of digital natives who bring a huge appetite-and aptitude-for the digital world?" His thoughts on how the Smithsonian is tackling this issue and how others have fared in museums and libraries around the world are the subject of a new e-book, Best of Both Worlds: Museums, Libraries, and Archives in a Digital Age.The e-book begins with a summary of what has already taken place in libraries and archives-documents once available only in the stacks or back rooms are now available, often for free, through the Internet. The Library of Congress and the National Archives, along with thousands of local libraries around the country, have led the way in digitizing two-dimensional objects.For museums, the digital world presents a bigger challenge. Clough cites several reasons for this. First, and most obvious: Producing images of three-dimensional objects is more complicated than taking a picture of a page and adding some data. Most museums, including the Smithsonian, previously have not had high levels of technical expertise and equipment. Adding to the challenge is the simple fact that collections are built with exhibitions in mind rather than open access on computers.With 137 million objects in its care, how did the Smithsonian begin the process of digitizing its vast collections to make them accessible to the millions of people who do not visit the museums in person? Clough describes the journey that began in 2009 with setting priorities for what would be digitized-a total of 14 million objects."The physical museum offers visitors the opportunity to experience the real object and share their impressions with family and friends," Clough said. "Digital access can then provide limitless opportunities for engagement and lifelong learning."Museums do not give degrees but they do provide informal education through their research, scholarship and exhibitions, both real and virtual. Clough sees museums gradually moving beyond showcasing collections to engaging the public online where the "visitors" can sort out and access the objects they find most interesting and then interact directly with the museums.Education has always been at the core of the Smithsonian. Today, the Smithsonian offers materials and lesson plans that meet state standards for K-12 curricula; online national summits for teens on subjects such as the 1961 Freedom Riders, the environment and the 1930s Dust Bowl; the Collections Search Center website; and apps such as Leafsnap that allow people to take a picture of a tree's leaf and have it identified in seconds. One of the most visible examples of informal education is the Smithsonian's website, www.seriouslyamazing.com, which draws people in with fun questions and, with another click, takes them deeper into the subject. For example, when the question "What European colonizer is still invading the U.S. today?" is clicked on, the answer, earthworms, appears along with further in-depth information on worms from environmental researchers.Museums have moved slowly-frequently project by project-into the brave new digital world, according to Clough. There are many good reasons for this cautious path, but "the time for toe-dipping is ending for museums, since they cannot stand aloof from the rising tide of information convergence."Clough concludes his 75-page online book with this thought: "While digital technology poses great challenges, it also offers great possibilities. For the Smithsonian and our nation's other museums, libraries and archives, today is a time when we can serve the role our founders envisioned for the educational systems of our republic. We can help all the people,},
  pagetotal = {130},
  publisher = {{Smithsonian Books}},
  date = {2013-09-17},
  author = {Clough, G. Wayne}
}

@online{StaatsbibliothekBerlin2016,
  title = {Staatsbibliothek Zu {{Berlin}}},
  url = {http://staatsbibliothek-berlin.de/},
  urldate = {2016-07-21},
  date = {2016},
  file = {/home/jakob/Zotero/storage/DJ42ZG6F/staatsbibliothek-berlin.de.html}
}

@article{VinyalsShowTellNeural2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4555},
  primaryClass = {cs},
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  url = {http://arxiv.org/abs/1411.4555},
  shorttitle = {Show and {{Tell}}},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  urldate = {2016-07-21},
  date = {2014-11-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  file = {/Users/jschmolling/Dropbox/ebooks/Vinyals_et_al_2014_Show_and_Tell.pdf;/home/jakob/Zotero/storage/FXSJIW82/1411.html}
}

@online{RodrigoBenensonClassificationdatasetsresults2016,
  title = {Classification Datasets Results},
  url = {https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html},
  urldate = {2016-07-21},
  date = {2016},
  author = {{Rodrigo Benenson}},
  file = {/home/jakob/Zotero/storage/EG8JSTE4/classification_datasets_results.html}
}

@online{WilhelmBuschMaxundMoritz1865,
  title = {Max Und {{Moritz}} : Eine {{Bubengeschichte}} in Sieben {{Streichen}}},
  url = {http://digital.staatsbibliothek-berlin.de/werkansicht?PPN=PPN644058722&PHYSID=PHYS_0007&DMDID=DMDLOG_0001},
  urldate = {2016-07-21},
  date = {1865},
  author = {{Wilhelm Busch}},
  file = {/home/jakob/Zotero/storage/DJMZQJEH/werkansicht.html}
}

@online{StaatsbibliothekzuBerlinDigitalisierteSammlungenStaatsbibliothek2016,
  title = {Digitalisierte {{Sammlungen}} Der {{Staatsbibliothek}} Zu {{Berlin}}: {{Startseite}}},
  url = {http://digital.staatsbibliothek-berlin.de/},
  urldate = {2016-07-21},
  date = {2016},
  author = {{\{Staatsbibliothek zu Berlin\}}},
  file = {/home/jakob/Zotero/storage/NNJUFW92/digital.staatsbibliothek-berlin.de.html}
}

@book{OgonyokSovetskoefotovyp1967,
  langid = {russian},
  title = {Советское фото, вып. 05, 1967 г.},
  url = {http://archive.org/details/sovphoto_v1_1967_05},
  abstract = {Советское фото
Журнал фотолюбительства и фоторепортажа
Sovetskoe Photo magazine
 выпуск/issue 05, год/year 1967},
  date = {1967},
  keywords = {landscape photography,magazine,nature,photography,Sovetskoe Foto,Soviet Photo,street photography,ussr,журнал,природа,Советское фото,СССР,фотография},
  author = {{Огонёк}}
}

@article{Santorosimpleneuralnetwork2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.01427},
  primaryClass = {cs},
  title = {A Simple Neural Network Module for Relational Reasoning},
  url = {http://arxiv.org/abs/1706.01427},
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  date = {2017-06-05},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  file = {/Users/jschmolling/Dropbox/ebooks/Santoro_et_al_2017_A_simple_neural_network_module_for_relational_reasoning.pdf;/home/jakob/Zotero/storage/64S6X7BT/1706.html}
}

@inproceedings{YangHierarchicalattentionnetworks2016,
  title = {Hierarchical Attention Networks for Document Classification},
  url = {http://www.aclweb.org/anthology/N16-1174},
  booktitle = {Proceedings of {{NAACL}}-{{HLT}}},
  date = {2016},
  pages = {1480--1489},
  author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  file = {/Users/jschmolling/Dropbox/ebooks/Yang_et_al_2016_Hierarchical_attention_networks_for_document_classification.pdf}
}

@article{JaderbergSyntheticDataArtificial2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2227},
  primaryClass = {cs},
  title = {Synthetic {{Data}} and {{Artificial Neural Networks}} for {{Natural Scene Text Recognition}}},
  url = {http://arxiv.org/abs/1406.2227},
  abstract = {In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.},
  date = {2014-06-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  file = {/Users/jschmolling/Dropbox/ebooks/Jaderberg_et_al_2014_Synthetic_Data_and_Artificial_Neural_Networks_for_Natural_Scene_Text_Recognition.pdf;/home/jakob/Zotero/storage/EQ4I8XVD/1406.html}
}

@article{ShiEndtoEndTrainableNeural2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.05717},
  primaryClass = {cs},
  title = {An {{End}}-to-{{End Trainable Neural Network}} for {{Image}}-Based {{Sequence Recognition}} and {{Its Application}} to {{Scene Text Recognition}}},
  url = {http://arxiv.org/abs/1507.05717},
  abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
  date = {2015-07-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
  file = {/Users/jschmolling/Dropbox/ebooks/Shi_et_al_2015_An_End-to-End_Trainable_Neural_Network_for_Image-based_Sequence_Recognition_and.pdf;/home/jakob/Zotero/storage/8GA7U8WG/1507.html},
  annotation = {https://github.com/bgshih/crnn

Comment: 5 figures}
}

@article{SimonyanVeryDeepConvolutional2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  urldate = {2017-05-22},
  date = {2014-09-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  file = {/Users/jschmolling/Dropbox/ebooks/Simonyan_Zisserman_2014_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition.pdf;/home/jakob/Zotero/storage/WIBPNC5S/1409.html}
}

@inproceedings{KangConvolutionalneuralnetworks2014,
  title = {Convolutional Neural Networks for Document Image Classification},
  url = {http://ieeexplore.ieee.org/abstract/document/6977258/},
  booktitle = {Pattern {{Recognition}} ({{ICPR}}), 2014 22nd {{International Conference}} On},
  publisher = {{IEEE}},
  urldate = {2017-05-15},
  date = {2014},
  pages = {3168--3172},
  author = {Kang, Le and Kumar, Jayant and Ye, Peng and Li, Yi and Doermann, David},
  file = {/Users/jschmolling/Dropbox/ebooks/Kang_et_al_2014_Convolutional_neural_networks_for_document_image_classification.pdf}
}

@thesis{UferMetauberwachtesLernenneuronaler2016,
  title = {Metaüberwachtes {{Lernen}} Neuronaler {{Netze}} Durch {{Modifizierung}} Des {{Backpropagation Algorithmus}}},
  url = {https://sisis.rz.htw-berlin.de/InfoGuideClient/singleHit.do?methodToCall=showHit&curPos=1&identifier=-1_S_IG.274.20737},
  urldate = {2017-05-01},
  date = {2016},
  author = {Ufer, Moritz},
  file = {/Users/jschmolling/Dropbox/ebooks/Ufer_2016_Metaüberwachtes_Lernen_neuronaler_Netze_durch_Modifizierung_des_Backpropagation.pdf;/home/jakob/Zotero/storage/CJV82R6F/singleHit.html}
}

@thesis{BielenEntwicklunglernfahigenAlogrithmus2015,
  title = {Entwicklung Eines Lernfähigen {{Alogrithmus}} Zur {{Auto}}-{{Kaufberatung}}},
  url = {https://sisis.rz.htw-berlin.de/InfoGuideClient/search.do?methodToCall=submit&CSId=7651N18S6381d24b8205ee20884e89c7890a060ce4b673ff&methodToCallParameter=submitSearch&searchCategories%5B0%5D=-1&searchString%5B0%5D=Bielen%2C+Sebastian&callingPage=searchParameters&selectedViewBranchlib=94&searchRestrictionID%5B0%5D=6&searchRestrictionValue1%5B0%5D=&searchRestrictionID%5B1%5D=5&searchRestrictionValue1%5B1%5D=&searchRestrictionID%5B2%5D=4&searchRestrictionValue1%5B2%5D=&searchRestrictionID%5B3%5D=2&searchRestrictionValue1%5B3%5D=},
  urldate = {2017-05-01},
  date = {2015},
  author = {Bielen, Sebastian},
  file = {/home/jakob/Zotero/storage/KGHXDR62/search.html}
}

@thesis{FelsbergTrainingRekurrentenNeuronalen2015,
  title = {Training von {{Rekurrenten Neuronalen Netzen}} Mittels {{GPGPU}}-{{Computing}}},
  url = {https://sisis.rz.htw-berlin.de/InfoGuideClient/login.do},
  urldate = {2017-05-01},
  date = {2015},
  author = {Felsberg, Norbert},
  file = {/home/jakob/Zotero/storage/5P2I45KZ/login.html}
}

@online{MyNeuralNetwork,
  title = {My {{Neural Network}} Isn't Working! {{What}} Should {{I}} Do?},
  url = {http://theorangeduck.com/page/neural-network-not-working},
  urldate = {2017-09-06},
  file = {/home/jakob/Zotero/storage/Z46U7YZF/neural-network-not-working.html}
}

@article{ChenDeepLabSemanticImage2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00915},
  primaryClass = {cs},
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  url = {http://arxiv.org/abs/1606.00915},
  shorttitle = {{{DeepLab}}},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  date = {2016-06-02},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen_et_al_2016_DeepLab.pdf;/home/jakob/Zotero/storage/QGDESVWN/1606.html},
  annotation = {Comment: Accepted by TPAMI}
}

@inproceedings{PapadopoulosIMPACTdatasethistorical2013,
  langid = {english},
  title = {The {{IMPACT}} Dataset of Historical Document Images},
  isbn = {978-1-4503-2115-0},
  doi = {10.1145/2501115.2501130},
  publisher = {{ACM Press}},
  date = {2013},
  pages = {123},
  author = {Papadopoulos, Christos and Pletschacher, Stefan and Clausner, Christian and Antonacopoulos, Apostolos},
  file = {/Users/jschmolling/Dropbox/ebooks/Papadopoulos_et_al_2013_The_IMPACT_dataset_of_historical_document_images.pdf}
}

@article{IoffeBatchNormalizationAccelerating2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  url = {http://arxiv.org/abs/1502.03167},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  date = {2015-02-10},
  keywords = {Computer Science - Learning},
  author = {Ioffe, Sergey and Szegedy, Christian},
  file = {/Users/jschmolling/Dropbox/ebooks/Ioffe_Szegedy_2015_Batch_Normalization.pdf;/home/jakob/Zotero/storage/7KSTVU9X/1502.html}
}

@article{DoerschWhatmakesparis2012,
  title = {What Makes Paris Look like Paris?},
  volume = {31},
  number = {4},
  journaltitle = {ACM Transactions on Graphics},
  date = {2012},
  author = {Doersch, Carl and Singh, Saurabh and Gupta, Abhinav and Sivic, Josef and Efros, Alexei},
  file = {/Users/jschmolling/Dropbox/ebooks/Doersch_et_al_2012_What_makes_paris_look_like_paris.pdf;/home/jakob/Zotero/storage/DP6BMC32/hal-01053876.html}
}

@inproceedings{RubinsteinUnsupervisedjointobject2013,
  title = {Unsupervised Joint Object Discovery and Segmentation in Internet Images},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  date = {2013},
  pages = {1939--1946},
  author = {Rubinstein, Michael and Joulin, Armand and Kopf, Johannes and Liu, Ce},
  file = {/Users/jschmolling/Dropbox/ebooks/Rubinstein_et_al_2013_Unsupervised_joint_object_discovery_and_segmentation_in_internet_images.pdf;/home/jakob/Zotero/storage/54JTGWIS/Rubinstein_Unsupervised_Joint_Object_2013_CVPR_paper.html}
}

@inproceedings{HenningEstimatingInformationGap2017,
  location = {{New York, NY, USA}},
  title = {Estimating the {{Information Gap Between Textual}} and {{Visual Representations}}},
  isbn = {978-1-4503-4701-3},
  doi = {10.1145/3078971.3078991},
  abstract = {Photos, drawings, figures, etc. supplement textual information in various kinds of media, for example, in web news or scientific publications. In this respect, the intended effect of an image can be quite different, e.g., providing additional information, focusing on certain details of surrounding text, or simply being a general illustration of a topic. As a consequence, the semantic correlation between information of different modalities can vary noticeably, too. Moreover, cross-modal interrelations are often hard to describe in a precise way. The variety of possible interrelations of textual and graphical information and the question, how they can be described and automatically estimated have not been addressed yet by previous work. In this paper, we present several contributions to close this gap. First, we introduce two measures to describe cross-modal interrelations: cross-modal mutual information (CMI) and semantic correlation (SC). Second, a novel approach relying on deep learning is suggested to estimate CMI and SC of textual and visual information. Third, three diverse datasets are leveraged to learn an appropriate deep neural network model for the demanding task. The system has been evaluated on a challenging test set and the experimental results demonstrate the feasibility of the approach.},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{International Conference}} on {{Multimedia Retrieval}}},
  series = {ICMR '17},
  publisher = {{ACM}},
  date = {2017},
  pages = {14--22},
  keywords = {deep learning,multimodal embeddings,text-image relations},
  author = {Henning, Christian Andreas and Ewerth, Ralph},
  file = {/Users/jschmolling/Dropbox/ebooks/Henning_Ewerth_2017_Estimating_the_Information_Gap_Between_Textual_and_Visual_Representations.pdf}
}

@inproceedings{ClausnerDocumentRepresentationRefinement2014,
  location = {{New York, NY, USA}},
  title = {Document {{Representation Refinement}} for {{Precise Region Description}}},
  isbn = {978-1-4503-2588-2},
  doi = {10.1145/2595188.2595198},
  abstract = {Precise description of layout entities (content regions on a page) is crucial for all but the most trivial document analysis and recognition applications. The output of layout analysis methods and state-of-the-art OCR systems varies significantly, from bounding boxes (e.g. Tesseract) to stacks of text line rectangles (e.g. ABBYY FineReader). There is a clear need for a consistent and accurate representation of regions (e.g. text paragraphs, graphics entities etc.) for further processing, correction and performance evaluation (comparison of segmentation results with ground truth regions). This paper describes a method for refinement of document representations by fitting polygons around lower-level layout objects (such as text lines, words and glyphs) in a systematic way that reconstructs region outlines and preserves the fine details of complex layouts. Experimental results on a standard dataset demonstrate the validity and usefulness of the proposed approach.},
  booktitle = {Proceedings of the {{First International Conference}} on {{Digital Access}} to {{Textual Cultural Heritage}}},
  series = {DATeCH '14},
  publisher = {{ACM}},
  date = {2014},
  pages = {9--13},
  keywords = {segmentation,document image analysis,polygonal fitting},
  author = {Clausner, Christian and Pletschacher, Stefan and Antonacopoulos, Apostolos}
}

@inproceedings{McConnaugheyLabeledSegmentationPrinted2017,
  title = {The {{Labeled Segmentation}} of {{Printed Books}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  date = {2017},
  pages = {748--758},
  author = {McConnaughey, Lara and Dai, Jennifer and Bamman, David},
  file = {/Users/jschmolling/Dropbox/ebooks/McConnaughey_et_al_2017_The_Labeled_Segmentation_of_Printed_Books.pdf}
}

@inproceedings{SanchezICDAR2015competition2015,
  title = {{{ICDAR}} 2015 Competition {{HTRtS}}: {{Handwritten Text Recognition}} on the {{tranScriptorium}} Dataset},
  doi = {10.1109/ICDAR.2015.7333944},
  shorttitle = {{{ICDAR}} 2015 Competition {{HTRtS}}},
  abstract = {This paper describes the second edition of the Handwritten Text Recognition (HTR) contest on the tranScriptorium datasets that has been held in the context of the International Conference on Document Analysis and Recognition 2015. Two tracks with different conditions on the use of training data were proposed. Nine research groups registered in the contest but finally three research submitted results. The handwritten images for this contest were drawn from the English “Bentham collection” dataset used in the tranScriptorium project. A small subset of this collection has been chosen for the present HTR competition. The selected subset has been written by several hands and entails significant variabilities and difficulties regarding the quality of text images, writing styles and crossed-out text. This contest is clearly more difficult than the the first edition both for training and for testing. A portion of the training dataset and the full test dataset were provided in the form of carefully segmented line images, along with the corresponding transcripts. Another portion of the training dataset was provided as raw images and their corresponding transcripts at region level. The three participants achieved good results, with transcription word error rates ranging from 31\% down to 44\%.},
  eventtitle = {2015 13th {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  booktitle = {2015 13th {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  date = {2015-08},
  pages = {1166--1170},
  keywords = {document image processing,Bentham collection dataset,handwriting recognition,handwritten text recognition,Hidden Markov models,Historical Documents,HTR,ICDAR 2015 competition HTRtS,image recognition,international conference on document analysis and recognition,natural language processing,Text analysis,text image quality,transcriptorium dataset,image segmentation},
  author = {Sánchez, J. A. and Toselli, A. H. and Romero, V. and Vidal, E.},
  file = {/Users/jschmolling/Dropbox/ebooks/Sánchez_et_al_2015_ICDAR_2015_competition_HTRtS.pdf;/home/jakob/Zotero/storage/PZXV3BZR/7333944.html}
}

@incollection{MarinaiIntroductionDocumentAnalysis2008,
  langid = {english},
  title = {Introduction to {{Document Analysis}} and {{Recognition}}},
  isbn = {978-3-540-76279-9 978-3-540-76280-5},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-76280-5_1},
  abstract = {Document Analysis and Recognition (DAR) aims at the automatic extraction of information presented on paper and initially addressed to human comprehension. The desired output of DAR systems is usually in a suitable symbolic representation that can subsequently be processed by computers.},
  booktitle = {Machine {{Learning}} in {{Document Analysis}} and {{Recognition}}},
  series = {Studies in Computational Intelligence},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-11-04},
  date = {2008},
  pages = {1--20},
  author = {Marinai, Simone},
  file = {/Users/jschmolling/Dropbox/ebooks/Marinai_2008_Introduction_to_Document_Analysis_and_Recognition.pdf;/home/jakob/Zotero/storage/PNTJRJ78/978-3-540-76280-5_1.html},
  doi = {10.1007/978-3-540-76280-5_1}
}

@online{ICDAR2017Document,
  title = {{{ICDAR}} 2017 {{Document Image Binarization Competition}} ({{DIBCO}} 2017)},
  url = {https://vc.ee.duth.gr/dibco2017/},
  urldate = {2017-11-04}
}

@article{VatsAutomaticDocumentImage2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01782},
  primaryClass = {cs},
  title = {Automatic {{Document Image Binarization}} Using {{Bayesian Optimization}}},
  url = {http://arxiv.org/abs/1709.01782},
  abstract = {Document image binarization is often a challenging task due to various forms of degradation. Although there exist several binarization techniques in literature, the binarized image is typically sensitive to control parameter settings of the employed technique. This paper presents an automatic document image binarization algorithm to segment the text from heavily degraded document images. The proposed technique uses a two band-pass filtering approach for background noise removal, and Bayesian optimization for automatic hyperparameter selection for optimal results. The effectiveness of the proposed binarization technique is empirically demonstrated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets.},
  date = {2017-09-06},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  author = {Vats, Ekta and Hast, Anders and Singh, Prashant},
  file = {/Users/jschmolling/Dropbox/ebooks/Vats_et_al_2017_Automatic_Document_Image_Binarization_using_Bayesian_Optimization.pdf;/home/jakob/Zotero/storage/6NNKJR4I/1709.html}
}

@online{ParzivalDatabaseComputer,
  title = {Parzival {{Database}} — {{Computer Vision}} and {{Artificial Intelligence}}},
  url = {http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/parzival-database},
  urldate = {2017-11-04}
}

@online{SaintGallDatabase,
  title = {Saint {{Gall Database}} — {{Computer Vision}} and {{Artificial Intelligence}}},
  url = {http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/saint-gall-database},
  urldate = {2017-11-04}
}

@article{GruningREADBADNewDataset2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.03311},
  primaryClass = {cs},
  title = {{{READ}}-{{BAD}}: {{A New Dataset}} and {{Evaluation Scheme}} for {{Baseline Detection}} in {{Archival Documents}}},
  url = {http://arxiv.org/abs/1705.03311},
  shorttitle = {{{READ}}-{{BAD}}},
  abstract = {Text line detection is crucial for any application associated with Automatic Text Recognition or Keyword Spotting. Modern algorithms perform good on well-established datasets since they either comprise clean data or simple/homogeneous page layouts. We have collected and annotated 2036 archival document images from different locations and time periods. The dataset contains varying page layouts and degradations that challenge text line segmentation methods. Well established text line segmentation evaluation schemes such as the Detection Rate or Recognition Accuracy demand for binarized data that is annotated on a pixel level. Producing groundtruth by these means is laborious and not needed to determine a method's quality. In this paper we propose a new evaluation scheme that is based on baselines. The proposed scheme has no need for binarization, it can handle skewed and rotated text lines and its results correlate with Handwritten Text Recognition accuracy. The ICDAR 2017 Competition on Baseline Detection and the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts make use of this evaluation scheme.},
  date = {2017-05-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Grüning, Tobias and Labahn, Roger and Diem, Markus and Kleber, Florian and Fiel, Stefan},
  file = {/Users/jschmolling/Dropbox/ebooks/Grüning_et_al_2017_READ-BAD.pdf;/home/jakob/Zotero/storage/F4SID7WE/1705.html},
  annotation = {Comment: Submitted to ICDAR2017}
}

@inproceedings{ClausnerDocumentRepresentationRefinement2014a,
  location = {{New York, NY, USA}},
  title = {Document {{Representation Refinement}} for {{Precise Region Description}}},
  isbn = {978-1-4503-2588-2},
  doi = {10.1145/2595188.2595198},
  abstract = {Precise description of layout entities (content regions on a page) is crucial for all but the most trivial document analysis and recognition applications. The output of layout analysis methods and state-of-the-art OCR systems varies significantly, from bounding boxes (e.g. Tesseract) to stacks of text line rectangles (e.g. ABBYY FineReader). There is a clear need for a consistent and accurate representation of regions (e.g. text paragraphs, graphics entities etc.) for further processing, correction and performance evaluation (comparison of segmentation results with ground truth regions). This paper describes a method for refinement of document representations by fitting polygons around lower-level layout objects (such as text lines, words and glyphs) in a systematic way that reconstructs region outlines and preserves the fine details of complex layouts. Experimental results on a standard dataset demonstrate the validity and usefulness of the proposed approach.},
  booktitle = {Proceedings of the {{First International Conference}} on {{Digital Access}} to {{Textual Cultural Heritage}}},
  series = {DATeCH '14},
  publisher = {{ACM}},
  date = {2014},
  pages = {9--13},
  keywords = {segmentation,document image analysis,polygonal fitting},
  author = {Clausner, Christian and Pletschacher, Stefan and Antonacopoulos, Apostolos},
  file = {/Users/jschmolling/Dropbox/ebooks/Clausner_et_al_2014_Document_Representation_Refinement_for_Precise_Region_Description.pdf}
}

@article{ChenConvolutionalNeuralNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.01474},
  primaryClass = {cs, stat},
  title = {Convolutional {{Neural Networks}} for {{Page Segmentation}} of {{Historical Document Images}}},
  url = {http://arxiv.org/abs/1704.01474},
  abstract = {This paper presents a Convolutional Neural Network (CNN) based page segmentation method for handwritten historical document images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.},
  date = {2017-04-05},
  keywords = {Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  author = {Chen, Kai and Seuret, Mathias},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen_Seuret_2017_Convolutional_Neural_Networks_for_Page_Segmentation_of_Historical_Document.pdf;/home/jakob/Zotero/storage/83RMJ6F6/1704.html}
}

@inproceedings{Al-BarhamtoshyUniversalmetadatarepository2016,
  title = {Universal Metadata Repository for Document Analysis and Recognition},
  doi = {10.1109/AICCSA.2016.7945622},
  abstract = {Document Analysis and Recognition (DAR) has two main objectives, first the analysis of the physical structure of the input image of the document, which should lead to the correct identification of the corresponding different homogeneous components and their boundaries in terms of XY coordinates. Second, each of these homogeneous components should be recognized in such a way that, if it is a text image, consequently this image should be recognized and translated into an intelligible text. DAR remains one of the most challenging topics in pattern recognition. Indeed, despite the diversity of the proposed approaches, techniques and methods, results remain very weak and away from expectations especially for several categories of documents such as complex, low quality, handwritten and historical documents. The complex structure and/or morphology of such documents are behind the weakness of results of these proposed approaches, techniques and methods. One of the challenging problems related to this topic is the creation of standard datasets that can be used by all stakeholders of this topic such as system developers, expert evaluators, and users. In addition, another challenging problem is how one could take advantages of all existing datasets that unfortunately are dispersed around the world without knowing, most of the times, any information about their locations and the way to reach them. As an attempt to solve the two mentioned above problems, we propose in this paper a Universal Datasets Repository for Document Analysis and Recognition (UMDAR) that has, in fact, a twofold advantage. First, it can help dataset creators to standardize their datasets and making them accessible to the research community once published on the proposed repository. Second, it can be used as a central which bridges in a smart manner between datasets and all DAR stakeholders.},
  eventtitle = {2016 {{IEEE}}/{{ACS}} 13th {{International Conference}} of {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  booktitle = {2016 {{IEEE}}/{{ACS}} 13th {{International Conference}} of {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  date = {2016-11},
  pages = {1--6},
  keywords = {document image processing,image recognition,Text analysis,Business,Databases,dataset,document analysis and recognition,document image,intelligible text,meta data,Metadata,pattern recognition,repositor,Standards,text image,Training,UMDAR,Universal Datasets Repository for Document Analysis and Recognition,universal metadata repository,image segmentation},
  author = {Al-Barhamtoshy, H. and Khemakhem, M. and Jambi, K. and Essa, F. and Fattouh, A. and Al-Ghamdi, A.},
  file = {/home/jakob/Zotero/storage/N5PMN25U/7945622.html}
}

@inproceedings{Joshisegmentationapproachbased2016,
  title = {A Segmentation Approach Based on Structured Learning for Recognition Preprocessing},
  doi = {10.1109/ICACDOT.2016.7877724},
  abstract = {Segmentation of manually written record photos into content material traces and words is a crucial undertaking for optical character acknowledgment. However, for the reason that elements of written by hand record are abnormal and various depending at the character, so far it is considered as a challenging issue. A good way to deal with the hassle, we formulate the word segmentation problem as a binary quadratic undertaking problem that considers pair-wise relationships among the crevices and in addition the probabilities of individual holes. No matter the fact that sever a parameters are covered in this detailing, we gauge all parameters taking into account the based Naive Bayes classifier so that the proposed technique features admirably paying little admire to composing styles and composed dialects without patron characterized parameters. Test effects on ICDAR 2009/2013 penmanship division data-set exhibit that proposed method accomplishes the high-quality in magnificence execution on Latin-primarily based and Indian languages.},
  eventtitle = {2016 {{International Conference}} on {{Automatic Control}} and {{Dynamic Optimization Techniques}} ({{ICACDOT}})},
  booktitle = {2016 {{International Conference}} on {{Automatic Control}} and {{Dynamic Optimization Techniques}} ({{ICACDOT}})},
  date = {2016-09},
  pages = {935--939},
  keywords = {Computers,Bayes methods,document image processing,electronic mail,natural language processing,binary quadratic,Classification algorithms,Dictionaries,handwritten character recognition,Handwritten documents,image classification,Indian language,Latin language,learning (artificial intelligence),naive Bayes classifier,optical character acknowledgment,optical character recognition,Optimization,pair-wise relationships,recognition preprocessing,structured learning,text detection,Transforms,word segmentation,word segmentation problem,written record photos,image segmentation},
  author = {Joshi, A. and Bharadwaj, D.},
  file = {/home/jakob/Zotero/storage/CA9EIFK2/7877724.html}
}

@article{SunRevisitingUnreasonableEffectiveness2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.02968},
  primaryClass = {cs},
  title = {Revisiting {{Unreasonable Effectiveness}} of {{Data}} in {{Deep Learning Era}}},
  url = {http://arxiv.org/abs/1707.02968},
  abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
  date = {2017-07-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence},
  author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  file = {/Users/jschmolling/Dropbox/ebooks/Sun_et_al_2017_Revisiting_Unreasonable_Effectiveness_of_Data_in_Deep_Learning_Era.pdf;/home/jakob/Zotero/storage/2T3QUK87/1707.html},
  annotation = {Comment: ICCV 2017 camera ready}
}

@article{DoerschUnsupervisedVisualRepresentation2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05192},
  primaryClass = {cs},
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  url = {http://arxiv.org/abs/1505.05192},
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
  date = {2015-05-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,self-supervised},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  file = {/Users/jschmolling/Dropbox/ebooks/Doersch_et_al_2015_Unsupervised_Visual_Representation_Learning_by_Context_Prediction.pdf;/home/jakob/Zotero/storage/A5KU63WN/1505.html},
  annotation = {Comment: Oral paper at ICCV 2015}
}

@article{HintonReducingdimensionalitydata2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  volume = {313},
  url = {http://science.sciencemag.org/content/313/5786/504.short},
  number = {5786},
  journaltitle = {science},
  date = {2006},
  pages = {504--507},
  author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
  file = {/Users/jschmolling/Dropbox/ebooks/Hinton_Salakhutdinov_2006_Reducing_the_dimensionality_of_data_with_neural_networks.pdf;/home/jakob/Zotero/storage/A6CTKW9S/504.html}
}

@article{ZhangUnderstandingdeeplearning2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03530},
  primaryClass = {cs},
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  url = {http://arxiv.org/abs/1611.03530},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  urldate = {2017-12-01},
  date = {2016-11-10},
  keywords = {Computer Science - Learning},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  file = {/Users/jschmolling/Dropbox/ebooks/Zhang et al_2016_Understanding deep learning requires rethinking generalization.pdf;/home/jakob/Zotero/storage/6SRD5HI4/1611.html},
  annotation = {Comment: Published in ICLR 2017}
}

@article{AchantaSLICSuperpixels2010,
  title = {{{SLIC Superpixels}}},
  url = {https://infoscience.epfl.ch/record/149300},
  urldate = {2017-12-01},
  date = {2010},
  author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurélien and Fua, Pascal and Süsstrunk, Sabine},
  file = {/Users/jschmolling/Dropbox/ebooks/Achanta et al_2010_SLIC Superpixels.pdf;/home/jakob/Zotero/storage/LE65VQW2/149300.html}
}

@inproceedings{ChenPageSegmentationHistorical2016,
  title = {Page {{Segmentation}} for {{Historical Document Images Based}} on {{Superpixel Classification}} with {{Unsupervised Feature Learning}}},
  doi = {10.1109/DAS.2016.13},
  abstract = {In this paper, we present an efficient page segmentation method for historical document images. Many existing methods either rely on hand-crafted features or perform rather slow as they treat the problem as a pixel-level assignment problem. In order to create a feasible method for real applications, we propose to use superpixels as basic units of segmentation, and features are learned directly from pixels. An image is first oversegmented into superpixels with the simple linear iterative clustering (SLIC) algorithm. Then, each superpixel is represented by the features of its central pixel. The features are learned from pixel intensity values with stacked convolutional autoencoders in an unsupervised manner. A support vector machine (SVM) classifier is used to classify superpixels into four classes: periphery, background, text block, and decoration. Finally, the segmentation results are refined by a connected component based smoothing procedure. Experiments on three public datasets demonstrate that compared to our previous method, the proposed method is much faster and achieves comparable segmentation results. Additionally, much fewer pixels are used for classifier training.},
  eventtitle = {2016 12th {{IAPR Workshop}} on {{Document Analysis Systems}} ({{DAS}})},
  booktitle = {2016 12th {{IAPR Workshop}} on {{Document Analysis Systems}} ({{DAS}})},
  date = {2016-04},
  pages = {299--304},
  keywords = {Training,Classification algorithms,image classification,autoencoder,central pixel,classifier training,Clustering algorithms,feature extraction,Feature extraction,hand-crafted features,historical document image,historical document images,history,Labeling,layout analysis,page segmentation,page segmentation method,pixel intensity values,pixel-level assignment problem,public datasets,simple linear iterative clustering algorithm,SLIC,smoothing procedure,stacked convolutional autoencoders,superpixel,superpixel classification,support vector machine classifier,support vector machines,Support vector machines,SVM,unsupervised feature learning,image segmentation},
  author = {Chen, K. and Liu, C. L. and Seuret, M. and Liwicki, M. and Hennebert, J. and Ingold, R.},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen et al_2016_Page Segmentation for Historical Document Images Based on Superpixel.pdf;/home/jakob/Zotero/storage/M772JVAN/7490134.html}
}

@video{AurelienGeronCapsuleNetworksCapsNets,
  title = {Capsule {{Networks}} ({{CapsNets}}) – {{Tutorial}}},
  url = {https://www.youtube.com/watch?v=pPN8d0E3900},
  keywords = {tutorial,neural networks,deep learning,capsnets,capsule networks,cifar10,computer vision,inverse graphics,mnist,object detection,routing by agreement,image segmentation},
  director = {{Aurélien Géron}}
}

@article{AlbertiHistoricalDocumentImage2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.07363},
  primaryClass = {cs},
  title = {Historical {{Document Image Segmentation}} with {{LDA}}-{{Initialized Deep Neural Networks}}},
  doi = {10.1145/3151509.3151519},
  abstract = {In this paper, we present a novel approach to perform deep neural networks layer-wise weight initialization using Linear Discriminant Analysis (LDA). Typically, the weights of a deep neural network are initialized with: random values, greedy layer-wise pre-training (usually as Deep Belief Network or as auto-encoder) or by re-using the layers from another network (transfer learning). Hence, many training epochs are needed before meaningful weights are learned, or a rather similar dataset is required for seeding a fine-tuning of transfer learning. In this paper, we describe how to turn an LDA into either a neural layer or a classification layer. We analyze the initialization technique on historical documents. First, we show that an LDA-based initialization is quick and leads to a very stable initialization. Furthermore, for the task of layout analysis at pixel level, we investigate the effectiveness of LDA-based initialization and show that it outperforms state-of-the-art random weight initialization methods.},
  date = {2017-10-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,diva},
  author = {Alberti, Michele and Seuret, Mathias and Pondenkandath, Vinaychandran and Ingold, Rolf and Liwicki, Marcus},
  file = {/Users/jschmolling/Dropbox/ebooks/Alberti_et_al_2017_Historical_Document_Image_Segmentation_with_LDA-Initialized_Deep_Neural_Networks.pdf;/home/jakob/Zotero/storage/KFBS5ZHW/1710.html},
  annotation = {Comment: 5 pages}
}

@inproceedings{SeuretPixelLevelHandwritten2014,
  title = {Pixel {{Level Handwritten}} and {{Printed Content Discrimination}} in {{Scanned Documents}}},
  doi = {10.1109/ICFHR.2014.77},
  abstract = {Classification of the content of a scanned document as either printed or handwritten is typically tackled as a segmentation problem of pages into text lines or words. However these methods are not applicable on documents where handwritten annotations overlay printed text. In this paper we propose to treat the task as a pixel classification task, i.e., To classify individual foreground pixels into either printed or handwritten pixels. Our method uses various features of diverse nature taking the surrounding window into account. The influence of the features and their parameters are investigated and optimized on a validation set. Each foreground pixel is then classified by a multilayer perceptron using feature vectors based on a pixel neighborhood. Finally, a post-processing step corrects typical misclassifications, i.e., It removes outliers based on several heuristics. We evaluated our method on printed documents with real handwritten annotations and reached an accuracy of 96.10\% on the test set. This is significantly higher than a previously published methods based on local features.},
  eventtitle = {2014 14th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}},
  booktitle = {2014 14th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}},
  date = {2014-09},
  pages = {423--428},
  keywords = {document image processing,Text analysis,Standards,Training,handwritten character recognition,image classification,Feature extraction,page segmentation,Accuracy,classification,content classification,feature vectors,handwritings,handwritten,handwritten annotations,Histograms,Image color analysis,multilayer perceptron,multilayer perceptrons,pixel classification task,printed,recognition,scanned documents,vectors,image segmentation},
  author = {Seuret, M. and Liwicki, M. and Ingold, R.},
  file = {/Users/jschmolling/Dropbox/ebooks/Seuret_et_al_2014_Pixel_Level_Handwritten_and_Printed_Content_Discrimination_in_Scanned_Documents.pdf;/home/jakob/Zotero/storage/UC9Z75IG/6981056.html}
}

@inproceedings{FischerTranscriptionalignmentLatin2011,
  langid = {english},
  title = {Transcription Alignment of {{Latin}} Manuscripts Using Hidden {{Markov}} Models},
  isbn = {978-1-4503-0916-5},
  doi = {10.1145/2037342.2037348},
  publisher = {{ACM Press}},
  date = {2011},
  pages = {29},
  keywords = {dataset},
  author = {Fischer, Andreas and Frinken, Volkmar and Fornés, Alicia and Bunke, Horst},
  annotation = {St. Gall Dataset

~}
}

@article{fischer_lexicon-free_2012,
  langid = {english},
  title = {Lexicon-Free Handwritten Word Spotting Using Character {{HMMs}}},
  volume = {33},
  issn = {01678655},
  doi = {10.1016/j.patrec.2011.09.009},
  number = {7},
  journaltitle = {Pattern Recognition Letters},
  date = {2012-05},
  pages = {934--942},
  keywords = {dataset},
  author = {Fischer, Andreas and Keller, Andreas and Frinken, Volkmar and Bunke, Horst},
  file = {/Users/jschmolling/Dropbox/ebooks/Fischer et al_2012_Lexicon-free handwritten word spotting using character HMMs.pdf},
  annotation = {Parzival Database

Washington Database}
}

@inproceedings{ChenGroundtruthmodel2015,
  title = {Ground Truth Model, Tool, and Dataset for Layout Analysis of Historical Documents},
  volume = {9402},
  doi = {10.1117/12.2075858},
  abstract = {In this paper, we propose a new dataset and a ground-truthing methodology for layout analysis of historical documents with complex layouts. The dataset is based on a generic model for ground-truth presentation of the complex layout structure of historical documents. For the purpose of extracting uniformly the document contents, our model defines five types of regions of interest:\emph{ page, text block, text line, decoration}, and \emph{comment}. Unconstrained polygons are used to outline the regions. A performance metric is proposed in order to evaluate various page segmentation methods based on this model. We have analysed four state-of-the-art ground-truthing tools: TRUVIZ, GEDI, WebGT, and Aletheia. From this analysis, we conceptualized and developed Divadia, a new tool that overcomes some of the drawbacks of these tools, targeting the simplicity and the efficiency of the layout ground truthing process on historical document images. With Divadia, we have created a new public dataset. This dataset contains 120 pages from three historical document image collections of different styles and is made freely available to the scientific community for historical document layout analysis research.},
  eventtitle = {Document {{Recognition}} and {{Retrieval XXII}}},
  publisher = {{International Society for Optics and Photonics}},
  date = {2015-02-08},
  pages = {940204},
  author = {Chen, Kai and Seuret, Mathias and Wei, Hao and Liwicki, Marcus and Hennebert, Jean and Ingold, Rolf},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen et al_2015_Ground truth model, tool, and dataset for layout analysis of historical.pdf;/home/jakob/Zotero/storage/XA4TWALU/12.2075858.html},
  annotation = {\href{http://diuf.unifr.ch/main/hisdoc/divadia}{http://diuf.unifr.ch/main/hisdoc/divadia}}
}

@inproceedings{ChenPageSegmentationHistorical2014,
  title = {Page {{Segmentation}} for {{Historical Handwritten Document Images Using Color}} and {{Texture Features}}},
  doi = {10.1109/ICFHR.2014.88},
  abstract = {In this paper we present a physical structure detection method for historical handwritten document images. We considered layout analysis as a pixel labeling problem. By classifying each pixel as either periphery, background, text block, or decoration, we achieve high quality segmentation without any assumption of specific topologies and shapes. Various color and texture features such as color variance, smoothness, Laplacian, Local Binary Patterns, and Gabor Dominant Orientation Histogram are used for classification. Some of these features have so far not got many attentions for document image layout analysis. By applying an Improved Fast Correlation-Based Filter feature selection algorithm, the redundant and irrelevant features are removed. Finally, the segmentation results are refined by a smoothing post-processing procedure. The proposed method is demonstrated by experiments conducted on three different historical handwritten document image datasets. Experiments show the benefit of combining various color and texture features for classification. The results also show the advantage of using a feature selection method to choose optimal feature subset. By applying the proposed method we achieve superior accuracy compared with earlier work on several datasets, e.g., We achieved 93\% accuracy compared with 91\% of the previous method on the Parzival dataset which contains about 100 million pixels.},
  eventtitle = {2014 14th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}},
  booktitle = {2014 14th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}},
  date = {2014-09},
  pages = {488--493},
  keywords = {document image processing,Text analysis,handwritten character recognition,layout analysis,page segmentation,Accuracy,Histograms,Image color analysis,Color,color variance,document image layout analysis,feature selection,feature selection method,Gabor dominant orientation histogram,high quality segmentation,historical document,historical handwritten document image datasets,historical handwritten document images,image segmentation,image texture,improved fast correlation-based filter feature selection algorithm,Layout,local binary patterns,Parzival dataset,pixel labeling problem,texture features},
  author = {Chen, K. and Wei, H. and Hennebert, J. and Ingold, R. and Liwicki, M.},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen et al_2014_Page Segmentation for Historical Handwritten Document Images Using Color and.pdf;/home/jakob/Zotero/storage/TCD3EPB3/6981067.html}
}

@book{CholletDeeplearningPython2018,
  langid = {english},
  location = {{Shelter Island, NY}},
  title = {Deep Learning with {{Python}}},
  isbn = {978-1-61729-443-3},
  publisher = {{Manning Publications Co.}},
  date = {2018},
  author = {Chollet, François},
  file = {/Users/jschmolling/Dropbox/ebooks/Chollet_2018_Deep_learning_with_Python.pdf},
  note = {OCLC: 982650571}
}

@article{KongRecurrentPixelEmbedding2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.08273},
  primaryClass = {cs},
  title = {Recurrent {{Pixel Embedding}} for {{Instance Grouping}}},
  url = {http://arxiv.org/abs/1712.08273},
  abstract = {We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on only correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.},
  date = {2017-12-21},
  keywords = {Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  author = {Kong, Shu and Fowlkes, Charless},
  file = {/Users/jschmolling/Dropbox/ebooks/Kong_Fowlkes_2017_Recurrent_Pixel_Embedding_for_Instance_Grouping.pdf;/home/jakob/Zotero/storage/4J9BT2I6/1712.html}
}

@article{NewlingKMedoidsKMeansSeeding2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04723},
  primaryClass = {cs},
  title = {K-{{Medoids For K}}-{{Means Seeding}}},
  url = {http://arxiv.org/abs/1609.04723},
  abstract = {We run experiments showing that algorithm clarans (Ng et al., 2005) finds better K-medoids solutions than the Voronoi iteration algorithm. This finding, along with the similarity between the Voronoi iteration algorithm and Lloyd's K-means algorithm, suggests that clarans may be an effective K-means initializer. We show that this is the case, with clarans outperforming other seeding algorithms on 23/23 datasets with a mean decrease over K-means++ of 30\% for initialization mse and 3\% or final mse. We describe how the complexity and runtime of clarans can be improved, making it a viable initialization scheme for large datasets.},
  date = {2016-09-15},
  keywords = {Computer Science - Data Structures and Algorithms},
  author = {Newling, James and Fleuret, François},
  file = {/Users/jschmolling/Dropbox/ebooks/Newling_Fleuret_2016_K-Medoids_For_K-Means_Seeding.pdf;/home/jakob/Zotero/storage/VHEPZE67/1609.html},
  annotation = {Comment: v1: (24 pages, 9 figures) v2: not at 33-rd ICML: forgot to modify .sty file. Reordered sections. Simplified to be specific to K-means seeding. New experiments. v3: (22 pages, 10 figures) Modified .sty file. Minor cosmetic changes. v4: added references and disussion of 2 related works v5: NIPS camera ready

\href{https://github.com/idiap/zentas}{https://github.com/idiap/zentas}

~}
}

@article{ChenCNNAllYou2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.09662},
  primaryClass = {cs},
  title = {{{CNN Is All You Need}}},
  url = {http://arxiv.org/abs/1712.09662},
  abstract = {The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learning; recently it has also challenged Recurrent Neural Networks (RNNs) with long short-term memory cells (LSTM) in sequence-to-sequence learning, since the computations involved in CNN are easily parallelizable whereas those involved in RNN are mostly sequential, leading to a performance bottleneck. However, unlike RNN, the native CNN lacks the history sensitivity required for sequence transformation; therefore enhancing the sequential order awareness, or position-sensitivity, becomes the key to make CNN the general deep learning model. In this work we introduce an extended CNN model with strengthen position-sensitivity, called PoseNet. A notable feature of PoseNet is the asymmetric treatment of position information in the encoder and the decoder. Experiments shows that PoseNet allows us to improve the accuracy of CNN based sequence-to-sequence learning significantly, achieving around 33-36 BLEU scores on the WMT 2014 English-to-German translation task, and around 44-46 BLEU scores on the English-to-French translation task.},
  date = {2017-12-27},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Chen, Qiming and Wu, Ren},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen_Wu_2017_CNN_Is_All_You_Need.pdf;/home/jakob/Zotero/storage/QB8WXRI5/1712.html}
}

@collection{DoermannHandbookdocumentimage2014,
  langid = {english},
  location = {{London}},
  title = {Handbook of Document Image Processing and Recognition},
  isbn = {978-0-85729-858-4 978-0-85729-859-1},
  shorttitle = {Handbook of Document Image Processing and Recognition},
  publisher = {{Springer Reference}},
  date = {2014},
  editor = {Doermann, David S.},
  file = {/Users/jschmolling/Dropbox/ebooks/Doermann_2014_Handbook_of_document_image_processing_and_recognition.pdf}
}

@online{ioannis_pratikakis_dibco_2017,
  title = {{{DIBCO}} 2017: {{ICDAR}} 2017 {{Document Image Binarization Competition}}},
  url = {https://vc.ee.duth.gr/dibco2017/},
  urldate = {2018-01-24},
  date = {2017},
  author = {{Ioannis Pratikakis} and {Konstantinos Zagoris} and {Georgios Barlas} and {Basilis Gatos}}
}

@inproceedings{SeuretNLightNHighlyAdaptableJava2016,
  title = {N-{{Light}}-{{N}}: {{A Highly}}-{{Adaptable Java Library}} for {{Document Analysis}} with {{Convolutional Auto}}-{{Encoders}} and {{Related Architectures}}},
  isbn = {978-1-5090-0981-7},
  doi = {10.1109/ICFHR.2016.0091},
  shorttitle = {N-{{Light}}-{{N}}},
  publisher = {{IEEE}},
  date = {2016-10},
  pages = {459--464},
  author = {Seuret, Mathias and Ingold, Rolf and Liwicki, Marcus},
  file = {/Users/jschmolling/Dropbox/ebooks/Seuret_et_al_2016_N-Light-N.pdf}
}

@inproceedings{SimistiraDIVAHisDBPreciselyAnnotated2016,
  title = {{{DIVA}}-{{HisDB}}: {{A Precisely Annotated Large Dataset}} of {{Challenging Medieval Manuscripts}}},
  isbn = {978-1-5090-0981-7},
  doi = {10.1109/ICFHR.2016.0093},
  shorttitle = {{{DIVA}}-{{HisDB}}},
  publisher = {{IEEE}},
  date = {2016-10},
  pages = {471--476},
  author = {Simistira, Foteini and Seuret, Mathias and Eichenberger, Nicole and Garz, Angelika and Liwicki, Marcus and Ingold, Rolf},
  file = {/Users/jschmolling/Dropbox/ebooks/Simistira_et_al_2016_DIVA-HisDB.pdf}
}

@article{DoerschMultitaskSelfSupervisedVisual2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.07860},
  primaryClass = {cs},
  title = {Multi-Task {{Self}}-{{Supervised Visual Learning}}},
  url = {http://arxiv.org/abs/1708.07860},
  abstract = {We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for "harmonizing" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.},
  date = {2017-08-25},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,self-supervised},
  author = {Doersch, Carl and Zisserman, Andrew},
  file = {/Users/jschmolling/Dropbox/ebooks/Doersch_Zisserman_2017_Multi-task_Self-Supervised_Visual_Learning.pdf;/home/jakob/Zotero/storage/6EESUVCA/1708.html},
  annotation = {Comment: Published at ICCV 2017}
}

@article{RadosavovicDataDistillationOmniSupervised2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.04440},
  primaryClass = {cs},
  title = {Data {{Distillation}}: {{Towards Omni}}-{{Supervised Learning}}},
  url = {http://arxiv.org/abs/1712.04440},
  shorttitle = {Data {{Distillation}}},
  abstract = {We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.},
  date = {2017-12-12},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Radosavovic, Ilija and Dollár, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming},
  file = {/Users/jschmolling/Dropbox/ebooks/Radosavovic_et_al_2017_Data_Distillation.pdf;/home/jakob/Zotero/storage/VRKACKCS/1712.html},
  annotation = {Comment: tech report}
}

@article{HuLearningSegmentEvery2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10370},
  primaryClass = {cs},
  title = {Learning to {{Segment Every Thing}}},
  url = {http://arxiv.org/abs/1711.10370},
  abstract = {Existing methods for object instance segmentation require all training instances to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to \textasciitilde{}100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models over a large set of categories for which all have box annotations, but only a small fraction have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We carefully evaluate our proposed approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.},
  date = {2017-11-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Hu, Ronghang and Dollár, Piotr and He, Kaiming and Darrell, Trevor and Girshick, Ross},
  file = {/Users/jschmolling/Dropbox/ebooks/Hu_et_al_2017_Learning_to_Segment_Every_Thing.pdf;/home/jakob/Zotero/storage/7S9GHVGH/1711.html}
}

@article{HeMaskRCNN2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06870},
  primaryClass = {cs},
  title = {Mask {{R}}-{{CNN}}},
  url = {http://arxiv.org/abs/1703.06870},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  date = {2017-03-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  file = {/Users/jschmolling/Dropbox/ebooks/He_et_al_2017_Mask_R-CNN.pdf;/home/jakob/Zotero/storage/7WQNCPDP/1703.html},
  annotation = {Comment: open source; appendix on more results}
}

@article{GoyalAccurateLargeMinibatch2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02677},
  primaryClass = {cs},
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  url = {http://arxiv.org/abs/1706.02677},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde{}90\% scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.},
  date = {2017-06-08},
  keywords = {Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  file = {/Users/jschmolling/Dropbox/ebooks/Goyal_et_al_2017_Accurate,_Large_Minibatch_SGD.pdf;/home/jakob/Zotero/storage/CHRREEP2/1706.html},
  annotation = {Comment: Tech report}
}

@article{NorooziUnsupervisedLearningVisual2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.09246},
  primaryClass = {cs},
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  url = {http://arxiv.org/abs/1603.09246},
  abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  date = {2016-03-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  file = {/Users/jschmolling/Dropbox/ebooks/Noroozi_Favaro_2016_Unsupervised_Learning_of_Visual_Representations_by_Solving_Jigsaw_Puzzles.pdf;/home/jakob/Zotero/storage/55ZKGE4P/1603.html},
  annotation = {Comment: ECCV 2016

Zusammenfassung

Der Vorteil des Jigsaw-Task ist, dass das Netz den gesamten Kontext einer Klasse verarbeiten kann.

Alle Bildpatches werden in relation zueinander gesehen.}
}

@online{RVLCDIPDataset,
  title = {{{RVL}}-{{CDIP Dataset}}},
  url = {https://www.cs.cmu.edu/~aharley/rvl-cdip/},
  urldate = {2018-02-15}
}

@article{CsurkaWhatrightway2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.01076},
  primaryClass = {cs},
  title = {What Is the Right Way to Represent Document Images?},
  url = {http://arxiv.org/abs/1603.01076},
  abstract = {In this article we study the problem of document image representation based on visual features. We propose a comprehensive experimental study that compares three types of visual document image representations: (1) traditional so-called shallow features, such as the RunLength and the Fisher-Vector descriptors, (2) deep features based on Convolutional Neural Networks, and (3) features extracted from hybrid architectures that take inspiration from the two previous ones. We evaluate these features in several tasks (i.e. classification, clustering, and retrieval) and in different setups (e.g. domain transfer) using several public and in-house datasets. Our results show that deep features generally outperform other types of features when there is no domain shift and the new task is closely related to the one used to train the model. However, when a large domain or task shift is present, the Fisher-Vector shallow features generalize better and often obtain the best results.},
  date = {2016-03-03},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Csurka, Gabriela and Larlus, Diane and Gordo, Albert and Almazan, Jon},
  file = {/Users/jschmolling/Dropbox/ebooks/Csurka_et_al_2016_What_is_the_right_way_to_represent_document_images.pdf;/home/jakob/Zotero/storage/84WISWJT/1603.html}
}

@inproceedings{JobinDocumentImageSegmentation2017,
  title = {Document {{Image Segmentation Using Deep Features}}},
  eventtitle = {National {{Conference}} on {{Computer Vision}}, {{Pattern Recognition}}, {{Image Processing}} and {{Graphics}} ({{NCVPRIPG}})},
  date = {2017},
  keywords = {diva},
  author = {Jobin, K. V. and Jawahar, C. V.},
  file = {/home/jakob/Zotero/storage/J5R4GXID/Jobin and Jawahar - Document Image Segmentation Using Deep.pdf;/home/jakob/Zotero/storage/XXAE8ZNV/Jobin and Jawahar - Document Image Segmentation Using Deep.pdf}
}

@article{WickFullyConvolutionalNeural2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.07695},
  primaryClass = {cs},
  title = {Fully {{Convolutional Neural Networks}} for {{Page Segmentation}} of {{Historical Document Images}}},
  url = {http://arxiv.org/abs/1711.07695},
  abstract = {We propose a high-performance fully convolutional neural network (FCN) for historical handwritten document segmentation that is designed to process a single page in one step. The advantage of this model beside its speed is its ability to directly learn from raw pixels instead of using preprocessing steps e. g. feature computation or superpixel generation. We show that this network yields better results than existing methods on different public data sets. For evaluation of this model we introduce a novel metric that is independent of ambiguous ground truth called Foreground Pixel Accuracy (FgPA). This pixel based measure only counts foreground pixels in the binarized page, any background pixel is omitted. The major advantage of this metric is, that it enables researchers to compare different segmentation methods on their ability to successfully segment text or pictures and not on their ability to learn and possibly overfit the peculiarities of an ambiguous hand-made ground truth segmentation.},
  date = {2017-11-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,diva},
  author = {Wick, Christoph and Puppe, Frank},
  file = {/Users/jschmolling/Dropbox/ebooks/Wick_Puppe_2017_Fully_Convolutional_Neural_Networks_for_Page_Segmentation_of_Historical.pdf;/home/jakob/Zotero/storage/GHFPMNV7/1711.html},
  annotation = {Comment: 6 pages, 8 figures, conference}
}

@article{RawatDeepConvolutionalNeural2017,
  langid = {english},
  title = {Deep {{Convolutional Neural Networks}} for {{Image Classification}}: {{A Comprehensive Review}}},
  volume = {29},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_00990},
  shorttitle = {Deep {{Convolutional Neural Networks}} for {{Image Classification}}},
  number = {9},
  journaltitle = {Neural Computation},
  date = {2017-09},
  pages = {2352--2449},
  author = {Rawat, Waseem and Wang, Zenghui},
  file = {/Users/jschmolling/Dropbox/ebooks/Rawat_Wang_2017_Deep_Convolutional_Neural_Networks_for_Image_Classification.pdf}
}

@online{HarryRansomCentera,
  title = {Harry {{Ransom Center Digital Collections}} - {{Harry Ransom Center Digital Collections}}},
  url = {https://hrc.contentdm.oclc.org/digital/},
  urldate = {2018-02-19},
  file = {/home/jakob/Zotero/storage/4ZNIGRB9/digital.html}
}

@article{RonnebergerUNetConvolutionalNetworks2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.04597},
  primaryClass = {cs},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  url = {http://arxiv.org/abs/1505.04597},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  urldate = {2018-02-20},
  date = {2015-05-18},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  file = {/Users/jschmolling/Dropbox/ebooks/Ronneberger_et_al_2015_U-Net.pdf;/home/jakob/Zotero/storage/MEIGRK8Z/1505.html},
  annotation = {Comment: conditionally accepted at MICCAI 2015}
}

@article{ChenChen2015Page2015,
  title = {Chen et Al. - 2015 - {{Page Segmentation}} of {{Historical Document Images}} with {{Convolutional Autoencoders}}},
  date = {2015-08-26},
  author = {Chen, Kai and Seuret, Mathias and Liwicki, Marcus and Hennebert, Jean and Ingold, Rolf},
  file = {/Users/jschmolling/Dropbox/ebooks/Chen_et_al_2015_Chen_et_al.pdf}
}

@incollection{BairdEvolutionDocumentImage2014,
  langid = {english},
  location = {{London}},
  title = {The {{Evolution}} of {{Document Image Analysis}}},
  isbn = {978-0-85729-858-4 978-0-85729-859-1},
  url = {http://link.springer.com/10.1007/978-0-85729-859-1_43},
  booktitle = {Handbook of {{Document Image Processing}} and {{Recognition}}},
  publisher = {{Springer London}},
  date = {2014},
  pages = {63--71},
  author = {Baird, Henry S. and Tombre, Karl},
  editor = {Doermann, David and Tombre, Karl},
  doi = {10.1007/978-0-85729-859-1_43}
}

@inproceedings{LongFullyconvolutionalnetworks2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  date = {2015},
  pages = {3431--3440},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  file = {/Users/jschmolling/Dropbox/ebooks/Long_et_al_2015_Fully_convolutional_networks_for_semantic_segmentation.pdf}
}

@online{IAMHistoricalDocument,
  title = {{{IAM Historical Document Database}} ({{IAM}}-{{HistDB}}) — {{Computer Vision}} and {{Artificial Intelligence}}},
  url = {http://www.fki.inf.unibe.ch/databases/iam-historical-document-database},
  urldate = {2018-03-02},
  file = {/home/jakob/Zotero/storage/J9EB7QQD/iam-historical-document-database.html}
}


