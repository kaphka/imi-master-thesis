
@online{staatsbibliothek_zu_berlin_digitalisierte_2016,
  title = {Digitalisierte {{Sammlungen}} der {{Staatsbibliothek}} zu {{Berlin}}: {{Startseite}}},
  url = {http://digital.staatsbibliothek-berlin.de/},
  timestamp = {2016-07-21T00:22:33Z},
  author = {{\{Staatsbibliothek zu Berlin\}}},
  urldate = {2016-07-21},
  date = {2016},
  file = {Digitalisierte Sammlungen der Staatsbibliothek zu Berlin\: Startseite:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/NNJUFW92/digital.staatsbibliothek-berlin.de.html:text/html}
}

@online{wilhelm_busch_max_1865,
  title = {Max und {{Moritz}} : eine {{Bubengeschichte}} in sieben {{Streichen}}},
  url = {http://digital.staatsbibliothek-berlin.de/werkansicht?PPN=PPN644058722&PHYSID=PHYS_0007&DMDID=DMDLOG_0001},
  timestamp = {2016-07-21T00:52:46Z},
  author = {{Wilhelm Busch}},
  urldate = {2016-07-21},
  date = {1865},
  file = {Digitalisierte Sammlungen der Staatsbibliothek zu Berlin\: Werkansicht - PPN644058722 - Übersicht mit Inhaltsverzeichnis:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DJMZQJEH/werkansicht.html:text/html}
}

@online{rodrigo_benenson_classification_2016,
  title = {Classification datasets results},
  url = {https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html},
  timestamp = {2016-07-21T09:59:34Z},
  author = {{Rodrigo Benenson}},
  urldate = {2016-07-21},
  date = {2016},
  file = {Classification datasets results:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/EG8JSTE4/classification_datasets_results.html:text/html}
}

@article{vinyals_show_2014,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  url = {http://arxiv.org/abs/1411.4555},
  shorttitle = {Show and {{Tell}}},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  timestamp = {2016-07-21T10:15:51Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4555},
  primaryClass = {cs},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  urldate = {2016-07-21},
  date = {2014-11-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Vinyals_et_al_2014_Show_and_Tell.pdf:/Users/jschmolling/Dropbox/ebooks/Vinyals_et_al_2014_Show_and_Tell.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/FXSJIW82/1411.html:text/html}
}

@online{_staatsbibliothek_2016,
  title = {Staatsbibliothek zu {{Berlin}}},
  url = {http://staatsbibliothek-berlin.de/},
  timestamp = {2016-07-21T12:59:54Z},
  urldate = {2016-07-21},
  date = {2016},
  file = {Startseite | Staatsbibliothek zu Berlin:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/DJ42ZG6F/staatsbibliothek-berlin.de.html:text/html}
}

@book{clough_best_2013,
  title = {Best of {{Both Worlds}}: {{Museums}}, {{Libraries}}, and {{Archives}} in the {{Digital Age}}},
  shorttitle = {Best of {{Both Worlds}}},
  abstract = {Wayne Clough, Secretary of the Smithsonian Institution, asks museums a fundamental question: "How can we prepare ourselves to reach the generation of digital natives who bring a huge appetite-and aptitude-for the digital world?" His thoughts on how the Smithsonian is tackling this issue and how others have fared in museums and libraries around the world are the subject of a new e-book, Best of Both Worlds: Museums, Libraries, and Archives in a Digital Age.The e-book begins with a summary of what has already taken place in libraries and archives-documents once available only in the stacks or back rooms are now available, often for free, through the Internet. The Library of Congress and the National Archives, along with thousands of local libraries around the country, have led the way in digitizing two-dimensional objects.For museums, the digital world presents a bigger challenge. Clough cites several reasons for this. First, and most obvious: Producing images of three-dimensional objects is more complicated than taking a picture of a page and adding some data. Most museums, including the Smithsonian, previously have not had high levels of technical expertise and equipment. Adding to the challenge is the simple fact that collections are built with exhibitions in mind rather than open access on computers.With 137 million objects in its care, how did the Smithsonian begin the process of digitizing its vast collections to make them accessible to the millions of people who do not visit the museums in person? Clough describes the journey that began in 2009 with setting priorities for what would be digitized-a total of 14 million objects."The physical museum offers visitors the opportunity to experience the real object and share their impressions with family and friends," Clough said. "Digital access can then provide limitless opportunities for engagement and lifelong learning."Museums do not give degrees but they do provide informal education through their research, scholarship and exhibitions, both real and virtual. Clough sees museums gradually moving beyond showcasing collections to engaging the public online where the "visitors" can sort out and access the objects they find most interesting and then interact directly with the museums.Education has always been at the core of the Smithsonian. Today, the Smithsonian offers materials and lesson plans that meet state standards for K-12 curricula; online national summits for teens on subjects such as the 1961 Freedom Riders, the environment and the 1930s Dust Bowl; the Collections Search Center website; and apps such as Leafsnap that allow people to take a picture of a tree's leaf and have it identified in seconds. One of the most visible examples of informal education is the Smithsonian's website, www.seriouslyamazing.com, which draws people in with fun questions and, with another click, takes them deeper into the subject. For example, when the question "What European colonizer is still invading the U.S. today?" is clicked on, the answer, earthworms, appears along with further in-depth information on worms from environmental researchers.Museums have moved slowly-frequently project by project-into the brave new digital world, according to Clough. There are many good reasons for this cautious path, but "the time for toe-dipping is ending for museums, since they cannot stand aloof from the rising tide of information convergence."Clough concludes his 75-page online book with this thought: "While digital technology poses great challenges, it also offers great possibilities. For the Smithsonian and our nation's other museums, libraries and archives, today is a time when we can serve the role our founders envisioned for the educational systems of our republic. We can help all the people,},
  pagetotal = {130},
  timestamp = {2016-07-21T19:32:35Z},
  langid = {english},
  publisher = {{Smithsonian Books}},
  author = {Clough, G. Wayne},
  date = {2013-09-17}
}

@online{_kitodo:_????,
  title = {Kitodo: {{Kitodo}}. {{Key}} to digital objects},
  url = {http://www.kitodo.org/},
  timestamp = {2016-07-22T00:32:42Z},
  urldate = {2016-07-22},
  file = {Kitodo\: Kitodo. Key to digital objects:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/ZHGDQ4QF/www.kitodo.org.html:text/html}
}

@article{razavian_cnn_2014,
  title = {{{CNN Features}} off-the-shelf: an {{Astounding Baseline}} for {{Recognition}}},
  url = {http://arxiv.org/abs/1403.6382},
  shorttitle = {{{CNN Features}} off-the-shelf},
  abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \ensuremath{\backslash}overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \ensuremath{\backslash}overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \ensuremath{\backslash}overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or \$L2\$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
  timestamp = {2016-07-24T23:46:05Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.6382},
  primaryClass = {cs},
  author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  urldate = {2016-07-24},
  date = {2014-03-23},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Razavian_et_al_2014_CNN_Features_off-the-shelf.pdf:/Users/jschmolling/Dropbox/ebooks/Razavian_et_al_2014_CNN_Features_off-the-shelf.pdf:application/pdf}
}

@article{girshick_region-based_2016,
  title = {Region-based convolutional networks for accurate object detection and segmentation},
  volume = {38},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7112511},
  timestamp = {2016-07-24T23:39:00Z},
  number = {1},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  urldate = {2016-07-24},
  date = {2016},
  pages = {142--158},
  keywords = {segmentation},
  file = {Girshick_et_al_2016_Region-based_convolutional_networks_for_accurate_object_detection_and.pdf:/Users/jschmolling/Dropbox/ebooks/Girshick_et_al_2016_Region-based_convolutional_networks_for_accurate_object_detection_and.pdf:application/pdf}
}

@article{ren_faster_2015,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  url = {http://arxiv.org/abs/1506.01497},
  shorttitle = {Faster {{R-CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  timestamp = {2016-07-25T07:58:31Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  urldate = {2016-07-25},
  date = {2015-06-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {Ren_et_al_2015_Faster_R-CNN.pdf:/Users/jschmolling/Dropbox/ebooks/Ren_et_al_2015_Faster_R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/8KW3TTPZ/1506.html:text/html}
}

@inreference{_digital_2016,
  title = {Digital {{Humanities}}},
  rights = {Creative Commons Attribution-ShareAlike License},
  url = {https://de.wikipedia.org/w/index.php?title=Digital_Humanities&oldid=156118661},
  abstract = {Das Fach Digital Humanities (deutsch: „digitale Geisteswissenschaften“) umfasst die Anwendung von computergestützten Verfahren und die systematische Verwendung von digitalen Ressourcen in den Geistes- und Kulturwissenschaften. Es handelt sich um ein interdisziplinär ausgerichtetes Fach, dessen Vertreter sowohl durch eine traditionelle Ausbildung in den Geistes- und Kulturwissenschaften ausgewiesen sind wie durch ihre Vertrautheit mit einer Reihe von einschlägigen Konzepten, Verfahren und Standards der Informatik. In Deutschland sind das insbesondere Forscher der Computerphilologie, der Historischen Fachinformatik, der Informationswissenschaft und der Computerlinguistik. Typische Arbeits- und Forschungsfelder sind z.B. digitale Editionen, quantitative Textanalyse, Visualisierung komplexer Datenstrukturen oder die Theorie digitaler Medien.},
  timestamp = {2016-07-25T08:29:40Z},
  langid = {german},
  booktitle = {Wikipedia},
  urldate = {2016-07-25},
  date = {2016-07-13},
  note = {Page Version ID: 156118661},
  file = {Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/GSQ4E2QK/index.html:text/html}
}

@article{harley_evaluation_2015,
  title = {Evaluation of {{Deep Convolutional Nets}} for {{Document Image Classification}} and {{Retrieval}}},
  url = {http://arxiv.org/abs/1502.07058},
  abstract = {This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.},
  timestamp = {2016-07-25T08:41:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.07058},
  primaryClass = {cs},
  author = {Harley, Adam W. and Ufkes, Alex and Derpanis, Konstantinos G.},
  urldate = {2016-07-25},
  date = {2015-02-25},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Harley_et_al_2015_Evaluation_of_Deep_Convolutional_Nets_for_Document_Image_Classification_and.pdf:/Users/jschmolling/Dropbox/ebooks/Harley_et_al_2015_Evaluation_of_Deep_Convolutional_Nets_for_Document_Image_Classification_and.pdf:application/pdf;arXiv.org Snapshot:/Users/jschmolling/Library/Application Support/Zotero/Profiles/kkvfcs3x.default/zotero/storage/EF48WJAS/1502.html:text/html}
}


