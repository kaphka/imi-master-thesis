{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(os.path.expanduser(\"~/.thesis.conf\")))\n",
    "db_folder = Path(config['datasets']) / Path(\"archive_org/\") \n",
    "identifier = \"pointlinetoplane00kand\"\n",
    "os.chdir(str(db_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchiveOrgDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.jp2 = glob(str(path / '*'/ '*' / '*.jp2'))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.jp2)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index < len(self.jp2)\n",
    "        img = Image.open(self.jp2[index])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "class RotateTask(object):\n",
    "    def __init__(self, interpolation=Image.BILINEAR,  resample=False, expand=False, center=None):\n",
    "        self.interpolation = interpolation\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = None\n",
    "        self.directions = {0: 0, 1: 90, 2: 180, 3: 270 }\n",
    "    def __call__(self, img):\n",
    "        d = random.randint(0,3)\n",
    "        rimg = img.rotate(self.directions[d], resample=Image.BILINEAR)\n",
    "        rimg = torchvision.transforms.functional.to_tensor(rimg)\n",
    "        return dict(x=rimg, label=d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "createRotateTask = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(64,scale=(0.8,1), ratio=(1,1)),\n",
    "    RotateTask()\n",
    "])\n",
    "aset = ArchiveOrgDataset(db_folder, transform=createRotateTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0, 'x': \n",
       " (0 ,.,.) = \n",
       "   0.8196  0.8275  0.8314  ...   0.7843  0.7804  0.7529\n",
       "   0.8196  0.8275  0.8314  ...   0.7843  0.7804  0.7569\n",
       "   0.8235  0.8314  0.8314  ...   0.7843  0.7804  0.7569\n",
       "            ...             ⋱             ...          \n",
       "   0.8118  0.8235  0.8196  ...   0.7922  0.7843  0.7765\n",
       "   0.8078  0.8157  0.8196  ...   0.7843  0.7843  0.7686\n",
       "   0.8039  0.8118  0.8118  ...   0.7804  0.7804  0.7686\n",
       " \n",
       " (1 ,.,.) = \n",
       "   0.7412  0.7529  0.7529  ...   0.7020  0.6941  0.6588\n",
       "   0.7412  0.7529  0.7529  ...   0.7059  0.6980  0.6627\n",
       "   0.7490  0.7529  0.7529  ...   0.7059  0.6941  0.6627\n",
       "            ...             ⋱             ...          \n",
       "   0.7176  0.7294  0.7373  ...   0.7098  0.6980  0.6824\n",
       "   0.7098  0.7216  0.7294  ...   0.7059  0.6902  0.6745\n",
       "   0.7059  0.7176  0.7294  ...   0.6980  0.6863  0.6706\n",
       " \n",
       " (2 ,.,.) = \n",
       "   0.6039  0.6118  0.6157  ...   0.5686  0.5569  0.5137\n",
       "   0.6039  0.6157  0.6157  ...   0.5686  0.5569  0.5137\n",
       "   0.6118  0.6157  0.6196  ...   0.5686  0.5529  0.5176\n",
       "            ...             ⋱             ...          \n",
       "   0.5686  0.5882  0.5961  ...   0.5608  0.5451  0.5333\n",
       "   0.5647  0.5765  0.5922  ...   0.5490  0.5373  0.5216\n",
       "   0.5569  0.5725  0.5804  ...   0.5490  0.5373  0.5216\n",
       " [torch.FloatTensor of size 3x64x64]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature: \u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m\n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\u001b[0m\n",
       "\u001b[0;34m    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns:\u001b[0m\n",
       "\u001b[0;34m            Tensor: Converted image.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mFile:           \u001b[0m~/anaconda/envs/ml-software/lib/python3.5/site-packages/torchvision/transforms/transforms.py\n",
       "\u001b[0;31mType:           \u001b[0mtype"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(aset,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
